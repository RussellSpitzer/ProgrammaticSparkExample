INFO  [main] 2017-06-12 12:37:17,436  GuavaCompatibility.java:126 - Detected Guava < 19 in the classpath, using legacy compatibility layer
INFO  [main] 2017-06-12 12:37:17,461  Native.java:106 - Could not load JNR C Library, native system calls through this library will not be available (set this logger level to DEBUG to see the full stack trace).
INFO  [main] 2017-06-12 12:37:17,461  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:37:17,563  YamlConfigurationLoader.java:89 - Configuration location: file:/Users/russellspitzer/repos/bdp/resources/cassandra/conf/cassandra.yaml
DEBUG [main] 2017-06-12 12:37:17,564  YamlConfigurationLoader.java:108 - Loading settings from file:/Users/russellspitzer/repos/bdp/resources/cassandra/conf/cassandra.yaml
INFO  [main] 2017-06-12 12:37:17,858  Config.java:487 - Node configuration:[allocate_tokens_for_keyspace=null; allocate_tokens_for_local_replication_factor=null; authenticator=com.datastax.bdp.cassandra.auth.DseAuthenticator; authorizer=com.datastax.bdp.cassandra.auth.DseAuthorizer; auto_bootstrap=true; auto_snapshot=true; back_pressure_enabled=false; back_pressure_strategy=org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}; batch_size_fail_threshold_in_kb=640; batch_size_warn_threshold_in_kb=64; batchlog_replay_throttle_in_kb=1024; broadcast_address=null; broadcast_rpc_address=null; buffer_pool_use_heap_if_exhausted=true; cas_contention_timeout_in_ms=1000; cdc_enabled=false; cdc_free_space_check_interval_ms=250; cdc_raw_directory=/var/lib/cassandra/cdc_raw; cdc_total_space_in_mb=0; client_encryption_options=<REDACTED>; cluster_name=Test Cluster; column_index_cache_size_in_kb=2; column_index_size_in_kb=64; commit_failure_policy=stop; commitlog_compression=null; commitlog_directory=/var/lib/cassandra/commitlog; commitlog_max_compression_buffers_in_pool=3; commitlog_periodic_queue_size=-1; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_batch_window_in_ms=NaN; commitlog_sync_period_in_ms=10000; commitlog_total_space_in_mb=null; compaction_large_partition_warning_threshold_mb=100; compaction_throughput_mb_per_sec=16; concurrent_compactors=null; concurrent_counter_writes=32; concurrent_materialized_view_writes=32; concurrent_reads=32; concurrent_replicates=null; concurrent_writes=32; continuous_paging=org.apache.cassandra.config.ContinuousPagingConfig@9caca531; counter_cache_keys_to_save=2147483647; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; credentials_cache_max_entries=1000; credentials_update_interval_in_ms=-1; credentials_validity_in_ms=2000; cross_node_timeout=false; data_file_directories=[Ljava.lang.String;@acb0951; disk_access_mode=auto; disk_failure_policy=stop; disk_optimization_estimate_percentile=0.95; disk_optimization_page_cross_chance=0.1; disk_optimization_strategy=ssd; dynamic_snitch=true; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; enable_scripted_user_defined_functions=false; enable_user_defined_functions=false; enable_user_defined_functions_threads=true; encryption_options=null; endpoint_snitch=com.datastax.bdp.snitch.DseSimpleSnitch; file_cache_size_in_mb=null; gc_log_threshold_in_ms=200; gc_warn_threshold_in_ms=1000; hinted_handoff_disabled_datacenters=[]; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; hints_compression=null; hints_directory=/var/lib/cassandra/hints; hints_flush_period_in_ms=10000; incremental_backups=false; index_interval=null; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=null; inter_dc_stream_throughput_outbound_megabits_per_sec=200; inter_dc_tcp_nodelay=false; internode_authenticator=null; internode_compression=dc; internode_recv_buff_size_in_bytes=0; internode_send_buff_size_in_bytes=0; key_cache_keys_to_save=2147483647; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=localhost; listen_interface=null; listen_interface_prefer_ipv6=false; listen_on_broadcast_address=false; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; max_hints_file_size_in_mb=128; max_mutation_size_in_kb=null; max_streaming_retries=3; max_value_size_in_mb=256; memtable_allocation_type=heap_buffers; memtable_cleanup_threshold=null; memtable_flush_writers=0; memtable_heap_space_in_mb=null; memtable_offheap_space_in_mb=null; min_free_space_per_drive_in_mb=50; native_transport_max_concurrent_connections=-1; native_transport_max_concurrent_connections_per_ip=-1; native_transport_max_frame_size_in_mb=256; native_transport_max_threads=128; native_transport_port=9042; native_transport_port_ssl=null; num_tokens=1; otc_backlog_expiration_interval_ms=200; otc_coalescing_enough_coalesced_messages=8; otc_coalescing_strategy=DISABLED; otc_coalescing_window_us=200; partitioner=org.apache.cassandra.dht.Murmur3Partitioner; permissions_cache_max_entries=1000; permissions_update_interval_in_ms=-1; permissions_validity_in_ms=2000; phi_convict_threshold=8.0; prepared_statements_cache_size_mb=null; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=5000; request_scheduler=org.apache.cassandra.scheduler.NoScheduler; request_scheduler_id=null; request_scheduler_options=null; request_timeout_in_ms=10000; role_manager=com.datastax.bdp.cassandra.auth.DseRoleManager; roles_cache_max_entries=1000; roles_update_interval_in_ms=-1; roles_validity_in_ms=2000; row_cache_class_name=org.apache.cassandra.cache.OHCProvider; row_cache_keys_to_save=2147483647; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=localhost; rpc_interface=null; rpc_interface_prefer_ipv6=false; rpc_keepalive=true; rpc_listen_backlog=50; rpc_max_threads=2147483647; rpc_min_threads=16; rpc_port=9160; rpc_recv_buff_size_in_bytes=null; rpc_send_buff_size_in_bytes=null; rpc_server_type=sync; saved_caches_directory=/var/lib/cassandra/saved_caches; seed_provider=org.apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1}; server_encryption_options=<REDACTED>; slow_query_log_timeout_in_ms=500; snapshot_before_compaction=false; ssl_storage_port=7001; sstable_preemptive_open_interval_in_mb=50; start_native_transport=true; start_rpc=true; storage_port=7000; stream_throughput_outbound_megabits_per_sec=200; streaming_keep_alive_period_in_secs=300; streaming_socket_timeout_in_ms=86400000; thrift_framed_transport_size_in_mb=15; thrift_max_message_length_in_mb=16; thrift_prepared_statements_cache_size_mb=null; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; tracetype_query_ttl=86400; tracetype_repair_ttl=604800; transparent_data_encryption_options=org.apache.cassandra.config.TransparentDataEncryptionOptions@5bf22f18; trickle_fsync=true; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=60000; unlogged_batch_across_partitions_warn_threshold=10; user_defined_function_fail_timeout=1500; user_defined_function_warn_timeout=500; user_function_timeout_policy=die; windows_timer_interval=1; write_request_timeout_in_ms=2000]
DEBUG [main] 2017-06-12 12:37:17,859  DatabaseDescriptor.java:358 - Syncing log with a period of 10000
INFO  [main] 2017-06-12 12:37:17,859  DatabaseDescriptor.java:366 - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  [main] 2017-06-12 12:37:17,859  DatabaseDescriptor.java:420 - Global memtable on-heap threshold is enabled at 57MB
INFO  [main] 2017-06-12 12:37:17,860  DatabaseDescriptor.java:424 - Global memtable off-heap threshold is enabled at 57MB
INFO  [main] 2017-06-12 12:37:17,979  RateBasedBackPressure.java:123 - Initialized back-pressure with high ratio: 0.9, factor: 5, flow: FAST, window size: 2000.
INFO  [main] 2017-06-12 12:37:17,980  DatabaseDescriptor.java:717 - Back-pressure is disabled with strategy org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}.
INFO  [main] 2017-06-12 12:37:18,029  DseConfigYamlLoader.java:38 - Loading settings from file:/Users/russellspitzer/repos/bdp/resources/dse/conf/dse.yaml
WARN  [main] 2017-06-12 12:37:18,030  DseConfigYamlLoader.java:43 - Incorrect configuration loader detected (default), DSE functionality may be impaired
INFO  [main] 2017-06-12 12:37:18,120  DseConfig.java:382 - Load of settings is done.
INFO  [main] 2017-06-12 12:37:18,120  DseConfig.java:402 - CQL slow log is enabled
INFO  [main] 2017-06-12 12:37:18,121  DseConfig.java:403 - CQL system info tables are not enabled
INFO  [main] 2017-06-12 12:37:18,121  DseConfig.java:404 - Resource level latency tracking is not enabled
INFO  [main] 2017-06-12 12:37:18,121  DseConfig.java:405 - Database summary stats are not enabled
INFO  [main] 2017-06-12 12:37:18,122  DseConfig.java:406 - Cluster summary stats are not enabled
INFO  [main] 2017-06-12 12:37:18,122  DseConfig.java:407 - Histogram data tables are not enabled
INFO  [main] 2017-06-12 12:37:18,122  DseConfig.java:408 - User level latency tracking is not enabled
INFO  [main] 2017-06-12 12:37:18,122  DseConfig.java:410 - Spark cluster info tables are not enabled
INFO  [main] 2017-06-12 12:37:18,123  DseConfig.java:444 - Cql solr query paging is: off
INFO  [main] 2017-06-12 12:37:18,125  DseUtil.java:503 - /proc/cpuinfo is not available, defaulting to 1 thread per CPU core...
INFO  [main] 2017-06-12 12:37:18,125  DseConfig.java:448 - This instance appears to have 1 thread per CPU core and 8 total CPU threads.
INFO  [main] 2017-06-12 12:37:18,126  DseConfig.java:465 - Server ID:34-36-3B-CC-92-D8
INFO  [main] 2017-06-12 12:37:18,129  Clock.java:46 - Using java.lang.System clock to generate timestamps.
WARN  [main] 2017-06-12 12:37:18,259  NettyUtil.java:64 - Found Netty's native epoll transport, but not running on linux-based operating system. Using NIO instead.
INFO  [main] 2017-06-12 12:37:18,655  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:37:18,657  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 12:37:18,813  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:37:18,929  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:37:18,930  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 12:37:18,964  Logging.scala:35 - Trying to setup a server socket at /10.150.0.180:64149 to verify connectivity with DSE node...
INFO  [main] 2017-06-12 12:37:18,991  Logging.scala:35 - Successfully verified DSE Node -> this application connectivity on random port (64149)
INFO  [main] 2017-06-12 12:37:19,031  Logging.scala:54 - Running Spark version 2.0.2.6
WARN  [main] 2017-06-12 12:37:19,157  NativeCodeLoader.java:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
INFO  [main] 2017-06-12 12:37:19,248  Logging.scala:54 - Changing view acls to: russellspitzer
INFO  [main] 2017-06-12 12:37:19,249  Logging.scala:54 - Changing modify acls to: russellspitzer
INFO  [main] 2017-06-12 12:37:19,249  Logging.scala:54 - Changing view acls groups to: 
INFO  [main] 2017-06-12 12:37:19,250  Logging.scala:54 - Changing modify acls groups to: 
INFO  [main] 2017-06-12 12:37:19,250  Logging.scala:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(russellspitzer); groups with view permissions: Set(); users  with modify permissions: Set(russellspitzer); groups with modify permissions: Set()
INFO  [main] 2017-06-12 12:37:19,325  Logging.scala:54 - Successfully started service 'sparkDriver' on port 51532.
INFO  [main] 2017-06-12 12:37:19,343  Logging.scala:54 - Registering MapOutputTracker
ERROR [main] 2017-06-12 12:37:19,368  Logging.scala:91 - Error initializing SparkContext.
java.lang.IllegalArgumentException: System memory 239075328 must be at least 471859200. Please increase heap size using the --driver-memory option or spark.driver.memory in Spark configuration.
	at org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:212) ~[spark-core_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:194) ~[spark-core_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:308) ~[spark-core_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:165) ~[spark-core_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:256) ~[spark-core_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:420) ~[spark-core_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2258) [spark-core_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$8.apply(SparkSession.scala:831) [spark-sql_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$8.apply(SparkSession.scala:823) [spark-sql_2.11-2.0.2.6.jar:2.0.2.6]
	at scala.Option.getOrElse(Option.scala:121) [na:na]
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:823) [spark-sql_2.11-2.0.2.6.jar:2.0.2.6]
	at com.datastax.spark.example.WriteRead$.delayedEndpoint$com$datastax$spark$example$WriteRead$1(WriteRead.scala:18) [writeread_2.11-0.1.jar:0.1]
	at com.datastax.spark.example.WriteRead$delayedInit$body.apply(WriteRead.scala:11) [writeread_2.11-0.1.jar:0.1]
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34) [na:na]
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12) [na:na]
	at scala.App$$anonfun$main$1.apply(App.scala:76) [na:na]
	at scala.App$$anonfun$main$1.apply(App.scala:76) [na:na]
	at scala.collection.immutable.List.foreach(List.scala:381) [na:na]
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35) [na:na]
	at scala.App$class.main(App.scala:76) [na:na]
	at com.datastax.spark.example.WriteRead$.main(WriteRead.scala:11) [writeread_2.11-0.1.jar:0.1]
	at com.datastax.spark.example.WriteRead.main(WriteRead.scala) [writeread_2.11-0.1.jar:0.1]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_60]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_60]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_60]
	at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_60]
	at scala.reflect.internal.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:70) [na:na]
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31) [na:na]
	at scala.reflect.internal.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:101) [na:na]
	at scala.reflect.internal.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:70) [na:na]
	at scala.reflect.internal.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101) [na:na]
	at scala.tools.nsc.CommonRunner$class.run(ObjectRunner.scala:22) [na:na]
	at scala.tools.nsc.JarRunner$.run(MainGenericRunner.scala:13) [na:na]
	at scala.tools.nsc.CommonRunner$class.runAndCatch(ObjectRunner.scala:29) [na:na]
	at scala.tools.nsc.JarRunner$.runJar(MainGenericRunner.scala:25) [na:na]
	at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:69) [na:na]
	at scala.tools.nsc.MainGenericRunner.run$1(MainGenericRunner.scala:87) [na:na]
	at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:98) [na:na]
	at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:103) [na:na]
	at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala) [na:na]
INFO  [main] 2017-06-12 12:37:19,373  Logging.scala:54 - Successfully stopped SparkContext
INFO  [Serial shutdown hooks thread] 2017-06-12 12:37:21,617  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [Serial shutdown hooks thread] 2017-06-12 12:37:23,834  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [Serial shutdown hooks thread] 2017-06-12 12:37:23,834  Logging.scala:35 - Successfully executed shutdown hook: Clearing session cache for C* connector
INFO  [main] 2017-06-12 12:38:02,521  GuavaCompatibility.java:126 - Detected Guava < 19 in the classpath, using legacy compatibility layer
INFO  [main] 2017-06-12 12:38:02,543  Native.java:106 - Could not load JNR C Library, native system calls through this library will not be available (set this logger level to DEBUG to see the full stack trace).
INFO  [main] 2017-06-12 12:38:02,544  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:38:02,647  YamlConfigurationLoader.java:89 - Configuration location: file:/Users/russellspitzer/repos/bdp/resources/cassandra/conf/cassandra.yaml
DEBUG [main] 2017-06-12 12:38:02,649  YamlConfigurationLoader.java:108 - Loading settings from file:/Users/russellspitzer/repos/bdp/resources/cassandra/conf/cassandra.yaml
INFO  [main] 2017-06-12 12:38:02,929  Config.java:487 - Node configuration:[allocate_tokens_for_keyspace=null; allocate_tokens_for_local_replication_factor=null; authenticator=com.datastax.bdp.cassandra.auth.DseAuthenticator; authorizer=com.datastax.bdp.cassandra.auth.DseAuthorizer; auto_bootstrap=true; auto_snapshot=true; back_pressure_enabled=false; back_pressure_strategy=org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}; batch_size_fail_threshold_in_kb=640; batch_size_warn_threshold_in_kb=64; batchlog_replay_throttle_in_kb=1024; broadcast_address=null; broadcast_rpc_address=null; buffer_pool_use_heap_if_exhausted=true; cas_contention_timeout_in_ms=1000; cdc_enabled=false; cdc_free_space_check_interval_ms=250; cdc_raw_directory=/var/lib/cassandra/cdc_raw; cdc_total_space_in_mb=0; client_encryption_options=<REDACTED>; cluster_name=Test Cluster; column_index_cache_size_in_kb=2; column_index_size_in_kb=64; commit_failure_policy=stop; commitlog_compression=null; commitlog_directory=/var/lib/cassandra/commitlog; commitlog_max_compression_buffers_in_pool=3; commitlog_periodic_queue_size=-1; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_batch_window_in_ms=NaN; commitlog_sync_period_in_ms=10000; commitlog_total_space_in_mb=null; compaction_large_partition_warning_threshold_mb=100; compaction_throughput_mb_per_sec=16; concurrent_compactors=null; concurrent_counter_writes=32; concurrent_materialized_view_writes=32; concurrent_reads=32; concurrent_replicates=null; concurrent_writes=32; continuous_paging=org.apache.cassandra.config.ContinuousPagingConfig@9caca531; counter_cache_keys_to_save=2147483647; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; credentials_cache_max_entries=1000; credentials_update_interval_in_ms=-1; credentials_validity_in_ms=2000; cross_node_timeout=false; data_file_directories=[Ljava.lang.String;@acb0951; disk_access_mode=auto; disk_failure_policy=stop; disk_optimization_estimate_percentile=0.95; disk_optimization_page_cross_chance=0.1; disk_optimization_strategy=ssd; dynamic_snitch=true; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; enable_scripted_user_defined_functions=false; enable_user_defined_functions=false; enable_user_defined_functions_threads=true; encryption_options=null; endpoint_snitch=com.datastax.bdp.snitch.DseSimpleSnitch; file_cache_size_in_mb=null; gc_log_threshold_in_ms=200; gc_warn_threshold_in_ms=1000; hinted_handoff_disabled_datacenters=[]; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; hints_compression=null; hints_directory=/var/lib/cassandra/hints; hints_flush_period_in_ms=10000; incremental_backups=false; index_interval=null; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=null; inter_dc_stream_throughput_outbound_megabits_per_sec=200; inter_dc_tcp_nodelay=false; internode_authenticator=null; internode_compression=dc; internode_recv_buff_size_in_bytes=0; internode_send_buff_size_in_bytes=0; key_cache_keys_to_save=2147483647; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=localhost; listen_interface=null; listen_interface_prefer_ipv6=false; listen_on_broadcast_address=false; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; max_hints_file_size_in_mb=128; max_mutation_size_in_kb=null; max_streaming_retries=3; max_value_size_in_mb=256; memtable_allocation_type=heap_buffers; memtable_cleanup_threshold=null; memtable_flush_writers=0; memtable_heap_space_in_mb=null; memtable_offheap_space_in_mb=null; min_free_space_per_drive_in_mb=50; native_transport_max_concurrent_connections=-1; native_transport_max_concurrent_connections_per_ip=-1; native_transport_max_frame_size_in_mb=256; native_transport_max_threads=128; native_transport_port=9042; native_transport_port_ssl=null; num_tokens=1; otc_backlog_expiration_interval_ms=200; otc_coalescing_enough_coalesced_messages=8; otc_coalescing_strategy=DISABLED; otc_coalescing_window_us=200; partitioner=org.apache.cassandra.dht.Murmur3Partitioner; permissions_cache_max_entries=1000; permissions_update_interval_in_ms=-1; permissions_validity_in_ms=2000; phi_convict_threshold=8.0; prepared_statements_cache_size_mb=null; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=5000; request_scheduler=org.apache.cassandra.scheduler.NoScheduler; request_scheduler_id=null; request_scheduler_options=null; request_timeout_in_ms=10000; role_manager=com.datastax.bdp.cassandra.auth.DseRoleManager; roles_cache_max_entries=1000; roles_update_interval_in_ms=-1; roles_validity_in_ms=2000; row_cache_class_name=org.apache.cassandra.cache.OHCProvider; row_cache_keys_to_save=2147483647; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=localhost; rpc_interface=null; rpc_interface_prefer_ipv6=false; rpc_keepalive=true; rpc_listen_backlog=50; rpc_max_threads=2147483647; rpc_min_threads=16; rpc_port=9160; rpc_recv_buff_size_in_bytes=null; rpc_send_buff_size_in_bytes=null; rpc_server_type=sync; saved_caches_directory=/var/lib/cassandra/saved_caches; seed_provider=org.apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1}; server_encryption_options=<REDACTED>; slow_query_log_timeout_in_ms=500; snapshot_before_compaction=false; ssl_storage_port=7001; sstable_preemptive_open_interval_in_mb=50; start_native_transport=true; start_rpc=true; storage_port=7000; stream_throughput_outbound_megabits_per_sec=200; streaming_keep_alive_period_in_secs=300; streaming_socket_timeout_in_ms=86400000; thrift_framed_transport_size_in_mb=15; thrift_max_message_length_in_mb=16; thrift_prepared_statements_cache_size_mb=null; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; tracetype_query_ttl=86400; tracetype_repair_ttl=604800; transparent_data_encryption_options=org.apache.cassandra.config.TransparentDataEncryptionOptions@5bf22f18; trickle_fsync=true; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=60000; unlogged_batch_across_partitions_warn_threshold=10; user_defined_function_fail_timeout=1500; user_defined_function_warn_timeout=500; user_function_timeout_policy=die; windows_timer_interval=1; write_request_timeout_in_ms=2000]
DEBUG [main] 2017-06-12 12:38:02,930  DatabaseDescriptor.java:358 - Syncing log with a period of 10000
INFO  [main] 2017-06-12 12:38:02,930  DatabaseDescriptor.java:366 - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  [main] 2017-06-12 12:38:02,931  DatabaseDescriptor.java:420 - Global memtable on-heap threshold is enabled at 455MB
INFO  [main] 2017-06-12 12:38:02,932  DatabaseDescriptor.java:424 - Global memtable off-heap threshold is enabled at 455MB
INFO  [main] 2017-06-12 12:38:03,071  RateBasedBackPressure.java:123 - Initialized back-pressure with high ratio: 0.9, factor: 5, flow: FAST, window size: 2000.
INFO  [main] 2017-06-12 12:38:03,072  DatabaseDescriptor.java:717 - Back-pressure is disabled with strategy org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}.
INFO  [main] 2017-06-12 12:38:03,133  DseConfigYamlLoader.java:38 - Loading settings from file:/Users/russellspitzer/repos/bdp/resources/dse/conf/dse.yaml
WARN  [main] 2017-06-12 12:38:03,134  DseConfigYamlLoader.java:43 - Incorrect configuration loader detected (default), DSE functionality may be impaired
INFO  [main] 2017-06-12 12:38:03,267  DseConfig.java:382 - Load of settings is done.
INFO  [main] 2017-06-12 12:38:03,269  DseConfig.java:402 - CQL slow log is enabled
INFO  [main] 2017-06-12 12:38:03,270  DseConfig.java:403 - CQL system info tables are not enabled
INFO  [main] 2017-06-12 12:38:03,271  DseConfig.java:404 - Resource level latency tracking is not enabled
INFO  [main] 2017-06-12 12:38:03,272  DseConfig.java:405 - Database summary stats are not enabled
INFO  [main] 2017-06-12 12:38:03,272  DseConfig.java:406 - Cluster summary stats are not enabled
INFO  [main] 2017-06-12 12:38:03,272  DseConfig.java:407 - Histogram data tables are not enabled
INFO  [main] 2017-06-12 12:38:03,273  DseConfig.java:408 - User level latency tracking is not enabled
INFO  [main] 2017-06-12 12:38:03,273  DseConfig.java:410 - Spark cluster info tables are not enabled
INFO  [main] 2017-06-12 12:38:03,274  DseConfig.java:444 - Cql solr query paging is: off
INFO  [main] 2017-06-12 12:38:03,276  DseUtil.java:503 - /proc/cpuinfo is not available, defaulting to 1 thread per CPU core...
INFO  [main] 2017-06-12 12:38:03,277  DseConfig.java:448 - This instance appears to have 1 thread per CPU core and 8 total CPU threads.
INFO  [main] 2017-06-12 12:38:03,279  DseConfig.java:465 - Server ID:34-36-3B-CC-92-D8
INFO  [main] 2017-06-12 12:38:03,284  Clock.java:46 - Using java.lang.System clock to generate timestamps.
WARN  [main] 2017-06-12 12:38:03,466  NettyUtil.java:64 - Found Netty's native epoll transport, but not running on linux-based operating system. Using NIO instead.
INFO  [main] 2017-06-12 12:38:03,948  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:38:03,949  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 12:38:04,102  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:38:04,175  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:38:04,176  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 12:38:04,198  Logging.scala:35 - Trying to setup a server socket at /10.150.0.180:55453 to verify connectivity with DSE node...
INFO  [main] 2017-06-12 12:38:04,216  Logging.scala:35 - Successfully verified DSE Node -> this application connectivity on random port (55453)
INFO  [main] 2017-06-12 12:38:04,257  Logging.scala:54 - Running Spark version 2.0.2.6
WARN  [main] 2017-06-12 12:38:04,407  NativeCodeLoader.java:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
INFO  [main] 2017-06-12 12:38:04,540  Logging.scala:54 - Changing view acls to: russellspitzer
INFO  [main] 2017-06-12 12:38:04,541  Logging.scala:54 - Changing modify acls to: russellspitzer
INFO  [main] 2017-06-12 12:38:04,541  Logging.scala:54 - Changing view acls groups to: 
INFO  [main] 2017-06-12 12:38:04,542  Logging.scala:54 - Changing modify acls groups to: 
INFO  [main] 2017-06-12 12:38:04,543  Logging.scala:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(russellspitzer); groups with view permissions: Set(); users  with modify permissions: Set(russellspitzer); groups with modify permissions: Set()
INFO  [main] 2017-06-12 12:38:04,631  Logging.scala:54 - Successfully started service 'sparkDriver' on port 51558.
INFO  [main] 2017-06-12 12:38:04,654  Logging.scala:54 - Registering MapOutputTracker
INFO  [main] 2017-06-12 12:38:04,675  Logging.scala:54 - Registering BlockManagerMaster
INFO  [main] 2017-06-12 12:38:04,692  Logging.scala:54 - Created local directory at /private/var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/blockmgr-f9464a3d-5d72-4ead-bd5f-5ef65ff9861f
INFO  [main] 2017-06-12 12:38:04,711  Logging.scala:54 - MemoryStore started with capacity 912.3 MB
INFO  [main] 2017-06-12 12:38:04,765  Logging.scala:54 - Registering OutputCommitCoordinator
INFO  [main] 2017-06-12 12:38:04,859  Log.java:186 - Logging initialized @3896ms
INFO  [main] 2017-06-12 12:38:04,955  Server.java:327 - jetty-9.2.z-SNAPSHOT
INFO  [main] 2017-06-12 12:38:04,975  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@522bf64e{/jobs,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,976  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@2aff9dff{/jobs/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,977  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5f0f9947{/jobs/job,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,977  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@1aad0b1{/jobs/job/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,978  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@c3edf4c{/stages,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,979  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5af8bb51{/stages/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,979  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@799ed4e8{/stages/stage,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,980  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@2e66bc32{/stages/stage/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,980  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@54d8c20d{/stages/pool,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,981  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4b65d9f4{/stages/pool/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,981  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@44536de4{/storage,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,982  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5fcfde70{/storage/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,982  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4d95a72e{/storage/rdd,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,983  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@28da7d11{/storage/rdd/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,983  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@77b919a3{/environment,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,984  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5624657a{/environment/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,984  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@36681447{/executors,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,985  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4d21c56e{/executors/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,986  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@726aa968{/executors/threadDump,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,986  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@7100dea{/executors/threadDump/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,995  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@712cfb63{/static,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,995  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@32e54a9d{/,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,996  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@15639440{/api,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:04,997  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@121bb45b{/stages/stage/kill,null,AVAILABLE}
INFO  [main] 2017-06-12 12:38:05,005  AbstractConnector.java:266 - Started ServerConnector@3d4e405e{HTTP/1.1}{0.0.0.0:4040}
INFO  [main] 2017-06-12 12:38:05,006  Server.java:379 - Started @4043ms
INFO  [main] 2017-06-12 12:38:05,007  Logging.scala:54 - Successfully started service 'SparkUI' on port 4040.
INFO  [main] 2017-06-12 12:38:05,010  Logging.scala:54 - Bound SparkUI to 0.0.0.0, and started at http://10.150.0.180:4040
INFO  [main] 2017-06-12 12:38:05,076  DseClusterManager.scala:28 - Creating task scheduler: 10.150.0.180:51558
INFO  [main] 2017-06-12 12:38:05,087  DseClusterManager.scala:33 - Creating scheduler backend
INFO  [main] 2017-06-12 12:38:05,095  DseClusterManager.scala:38 - Initialization
INFO  [main] 2017-06-12 12:38:05,128  Logging.scala:54 - Using connector configuration: CassandraConnectorConf(Set(/127.0.0.1),9042,NoAuthConf,None,11000,1000,4000,None,,10,5000,120000,com.datastax.spark.connector.cql.DefaultConnectionFactory$@97e93f1,CassandraSSLConf(false,None,None,JKS,TLS,Set(TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA),false,None,None,JKS))
INFO  [dse-app-client-thread-pool-1] 2017-06-12 12:38:05,133  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [dse-app-client-thread-pool-1] 2017-06-12 12:38:05,204  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [dse-app-client-thread-pool-1] 2017-06-12 12:38:05,204  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
WARN  [cluster3-nio-worker-2] 2017-06-12 12:38:05,237  RequestHandler.java:568 - /127.0.0.1:9042 replied with server error (Failed to execute method DseResourceManager.registerApplication), defuncting connection.
WARN  [cluster3-nio-worker-3] 2017-06-12 12:38:10,264  RequestHandler.java:568 - /127.0.0.1:9042 replied with server error (Failed to execute method DseResourceManager.registerApplication), defuncting connection.
INFO  [pool-1-thread-1] 2017-06-12 12:38:11,489  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
WARN  [cluster3-nio-worker-4] 2017-06-12 12:38:15,277  RequestHandler.java:568 - /127.0.0.1:9042 replied with server error (Failed to execute method DseResourceManager.registerApplication), defuncting connection.
INFO  [pool-1-thread-1] 2017-06-12 12:38:17,376  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
ERROR [dse-app-client-thread-pool-2] 2017-06-12 12:38:20,290  Logging.scala:91 - Failed to connect to DSE resource manager
java.io.IOException: Failed to register with master: dse://?
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint.org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster(DseAppClient.scala:98) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster$1.apply$mcV$sp(DseAppClient.scala:93) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster$1.apply(DseAppClient.scala:93) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster$1.apply(DseAppClient.scala:93) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[na:na]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[na:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_60]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_60]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]
Caused by: com.datastax.driver.core.exceptions.ServerError: An unexpected error occurred server side on /127.0.0.1:9042: Failed to execute method DseResourceManager.registerApplication
	at com.datastax.driver.core.exceptions.ServerError.copy(ServerError.java:54) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.exceptions.ServerError.copy(ServerError.java:16) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:28) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:236) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:59) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:42) ~[dse-java-driver-core-1.2.2.jar:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_60]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_60]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_60]
	at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_60]
	at com.datastax.spark.connector.cql.SessionProxy.invoke(SessionProxy.scala:40) ~[spark-cassandra-connector-unshaded_2.11-2.0.2.jar:2.0.2]
	at com.sun.proxy.$Proxy7.execute(Unknown Source) ~[na:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_60]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_60]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_60]
	at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_60]
	at com.datastax.spark.connector.cql.SessionProxy.invoke(SessionProxy.scala:40) ~[spark-cassandra-connector-unshaded_2.11-2.0.2.jar:2.0.2]
	at com.sun.proxy.$Proxy8.execute(Unknown Source) ~[na:na]
	at com.datastax.bdp.util.rpc.RpcUtil.call(RpcUtil.java:42) ~[dse-core-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseResourceManagerRPCClient$Impl.registerApplication(DseResourceManagerRPCClient.scala:55) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster$2.apply(DseAppClient.scala:106) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster$2.apply(DseAppClient.scala:105) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:112) ~[spark-cassandra-connector-unshaded_2.11-2.0.2.jar:2.0.2]
	at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:111) ~[spark-cassandra-connector-unshaded_2.11-2.0.2.jar:2.0.2]
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:145) ~[spark-cassandra-connector-unshaded_2.11-2.0.2.jar:2.0.2]
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:111) ~[spark-cassandra-connector-unshaded_2.11-2.0.2.jar:2.0.2]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint.org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster(DseAppClient.scala:105) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$2.apply(DseAppClient.scala:91) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$2.apply(DseAppClient.scala:91) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at scala.util.Try$.apply(Try.scala:192) ~[na:na]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint.org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster(DseAppClient.scala:91) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	... 8 common frames omitted
Caused by: com.datastax.driver.core.exceptions.ServerError: An unexpected error occurred server side on /127.0.0.1:9042: Failed to execute method DseResourceManager.registerApplication
	at com.datastax.driver.core.Responses$Error.asException(Responses.java:114) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:498) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1074) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:991) ~[dse-java-driver-core-1.2.2.jar:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:267) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	... 1 common frames omitted
ERROR [dse-app-client-thread-pool-2] 2017-06-12 12:38:20,292  Logging.scala:70 - Application has been killed. Reason: Failed to connect to DSE resource manager: Failed to register with master: dse://?
WARN  [main] 2017-06-12 12:38:20,292  Logging.scala:66 - Application ID is not initialized yet.
INFO  [main] 2017-06-12 12:38:20,298  Logging.scala:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51565.
INFO  [stop-spark-context] 2017-06-12 12:38:20,298  AbstractConnector.java:306 - Stopped ServerConnector@3d4e405e{HTTP/1.1}{0.0.0.0:4040}
INFO  [main] 2017-06-12 12:38:20,299  Logging.scala:54 - Server created on 10.150.0.180:51565
INFO  [stop-spark-context] 2017-06-12 12:38:20,300  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@121bb45b{/stages/stage/kill,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,300  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@15639440{/api,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,301  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@32e54a9d{/,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,301  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@712cfb63{/static,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,301  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@7100dea{/executors/threadDump/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,301  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@726aa968{/executors/threadDump,null,UNAVAILABLE}
INFO  [main] 2017-06-12 12:38:20,301  Logging.scala:54 - external shuffle service port = 7437
INFO  [stop-spark-context] 2017-06-12 12:38:20,301  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4d21c56e{/executors/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,302  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@36681447{/executors,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,302  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5624657a{/environment/json,null,UNAVAILABLE}
INFO  [main] 2017-06-12 12:38:20,302  Logging.scala:54 - Registering BlockManager BlockManagerId(driver, 10.150.0.180, 51565)
INFO  [stop-spark-context] 2017-06-12 12:38:20,302  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@77b919a3{/environment,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,302  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@28da7d11{/storage/rdd/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,303  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4d95a72e{/storage/rdd,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,303  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5fcfde70{/storage/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,303  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@44536de4{/storage,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,303  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4b65d9f4{/stages/pool/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,304  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@54d8c20d{/stages/pool,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,304  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@2e66bc32{/stages/stage/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,304  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@799ed4e8{/stages/stage,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,304  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5af8bb51{/stages/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,304  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@c3edf4c{/stages,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,305  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@1aad0b1{/jobs/job/json,null,UNAVAILABLE}
INFO  [dispatcher-event-loop-2] 2017-06-12 12:38:20,305  Logging.scala:54 - Registering block manager 10.150.0.180:51565 with 912.3 MB RAM, BlockManagerId(driver, 10.150.0.180, 51565)
INFO  [stop-spark-context] 2017-06-12 12:38:20,305  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5f0f9947{/jobs/job,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,305  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@2aff9dff{/jobs/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,306  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@522bf64e{/jobs,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:38:20,307  Logging.scala:54 - Stopped Spark web UI at http://10.150.0.180:4040
INFO  [main] 2017-06-12 12:38:20,308  Logging.scala:54 - Registered BlockManager BlockManagerId(driver, 10.150.0.180, 51565)
INFO  [stop-spark-context] 2017-06-12 12:38:20,316  Logging.scala:54 - Shutting down all executors
INFO  [dispatcher-event-loop-3] 2017-06-12 12:38:20,332  Logging.scala:54 - Asking each executor to shut down
INFO  [dispatcher-event-loop-6] 2017-06-12 12:38:20,448  Logging.scala:54 - Shutting down the thread pool
INFO  [dispatcher-event-loop-6] 2017-06-12 12:38:20,451  Logging.scala:54 - Evicting connector cache
INFO  [dispatcher-event-loop-1] 2017-06-12 12:38:20,461  Logging.scala:54 - MapOutputTrackerMasterEndpoint stopped!
INFO  [stop-spark-context] 2017-06-12 12:38:20,471  Logging.scala:54 - MemoryStore cleared
INFO  [stop-spark-context] 2017-06-12 12:38:20,472  Logging.scala:54 - BlockManager stopped
INFO  [stop-spark-context] 2017-06-12 12:38:20,479  Logging.scala:54 - BlockManagerMaster stopped
INFO  [dispatcher-event-loop-7] 2017-06-12 12:38:20,482  Logging.scala:54 - OutputCommitCoordinator stopped!
ERROR [main] 2017-06-12 12:38:20,492  Logging.scala:91 - Error initializing SparkContext.
java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem
	at scala.Predef$.require(Predef.scala:219) ~[na:na]
	at org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:90) ~[spark-core_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:509) ~[spark-core_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2258) [spark-core_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$8.apply(SparkSession.scala:831) [spark-sql_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$8.apply(SparkSession.scala:823) [spark-sql_2.11-2.0.2.6.jar:2.0.2.6]
	at scala.Option.getOrElse(Option.scala:121) [na:na]
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:823) [spark-sql_2.11-2.0.2.6.jar:2.0.2.6]
	at com.datastax.spark.example.WriteRead$.delayedEndpoint$com$datastax$spark$example$WriteRead$1(WriteRead.scala:18) [writeread_2.11-0.1.jar:0.1]
	at com.datastax.spark.example.WriteRead$delayedInit$body.apply(WriteRead.scala:11) [writeread_2.11-0.1.jar:0.1]
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34) [na:na]
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12) [na:na]
	at scala.App$$anonfun$main$1.apply(App.scala:76) [na:na]
	at scala.App$$anonfun$main$1.apply(App.scala:76) [na:na]
	at scala.collection.immutable.List.foreach(List.scala:381) [na:na]
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35) [na:na]
	at scala.App$class.main(App.scala:76) [na:na]
	at com.datastax.spark.example.WriteRead$.main(WriteRead.scala:11) [writeread_2.11-0.1.jar:0.1]
	at com.datastax.spark.example.WriteRead.main(WriteRead.scala) [writeread_2.11-0.1.jar:0.1]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_60]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_60]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_60]
	at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_60]
	at scala.reflect.internal.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:70) [na:na]
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31) [na:na]
	at scala.reflect.internal.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:101) [na:na]
	at scala.reflect.internal.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:70) [na:na]
	at scala.reflect.internal.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101) [na:na]
	at scala.tools.nsc.CommonRunner$class.run(ObjectRunner.scala:22) [na:na]
	at scala.tools.nsc.JarRunner$.run(MainGenericRunner.scala:13) [na:na]
	at scala.tools.nsc.CommonRunner$class.runAndCatch(ObjectRunner.scala:29) [na:na]
	at scala.tools.nsc.JarRunner$.runJar(MainGenericRunner.scala:25) [na:na]
	at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:69) [na:na]
	at scala.tools.nsc.MainGenericRunner.run$1(MainGenericRunner.scala:87) [na:na]
	at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:98) [na:na]
	at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:103) [na:na]
	at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala) [na:na]
INFO  [main] 2017-06-12 12:38:20,493  Logging.scala:54 - SparkContext already stopped.
INFO  [Thread-2] 2017-06-12 12:38:20,498  Logging.scala:54 - Shutdown hook called
INFO  [Serial shutdown hooks thread] 2017-06-12 12:38:20,498  Logging.scala:35 - Successfully executed shutdown hook: Clearing session cache for C* connector
INFO  [Thread-2] 2017-06-12 12:38:20,499  Logging.scala:54 - Deleting directory /private/var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/spark-45295a2a-c2a6-4a66-9a6a-874290fd7854
INFO  [Thread-2] 2017-06-12 12:38:20,500  Logging.scala:54 - Deleting directory /private/var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/spark-45295a2a-c2a6-4a66-9a6a-874290fd7854/userFiles-3daa333d-e408-4343-894c-9c48a8dc71dd
INFO  [main] 2017-06-12 12:42:36,639  GuavaCompatibility.java:126 - Detected Guava < 19 in the classpath, using legacy compatibility layer
INFO  [main] 2017-06-12 12:42:36,663  Native.java:106 - Could not load JNR C Library, native system calls through this library will not be available (set this logger level to DEBUG to see the full stack trace).
INFO  [main] 2017-06-12 12:42:36,663  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:42:36,776  YamlConfigurationLoader.java:89 - Configuration location: file:/Users/russellspitzer/repos/bdp/resources/cassandra/conf/cassandra.yaml
DEBUG [main] 2017-06-12 12:42:36,778  YamlConfigurationLoader.java:108 - Loading settings from file:/Users/russellspitzer/repos/bdp/resources/cassandra/conf/cassandra.yaml
INFO  [main] 2017-06-12 12:42:37,077  Config.java:487 - Node configuration:[allocate_tokens_for_keyspace=null; allocate_tokens_for_local_replication_factor=null; authenticator=com.datastax.bdp.cassandra.auth.DseAuthenticator; authorizer=com.datastax.bdp.cassandra.auth.DseAuthorizer; auto_bootstrap=true; auto_snapshot=true; back_pressure_enabled=false; back_pressure_strategy=org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}; batch_size_fail_threshold_in_kb=640; batch_size_warn_threshold_in_kb=64; batchlog_replay_throttle_in_kb=1024; broadcast_address=null; broadcast_rpc_address=null; buffer_pool_use_heap_if_exhausted=true; cas_contention_timeout_in_ms=1000; cdc_enabled=false; cdc_free_space_check_interval_ms=250; cdc_raw_directory=/var/lib/cassandra/cdc_raw; cdc_total_space_in_mb=0; client_encryption_options=<REDACTED>; cluster_name=Test Cluster; column_index_cache_size_in_kb=2; column_index_size_in_kb=64; commit_failure_policy=stop; commitlog_compression=null; commitlog_directory=/var/lib/cassandra/commitlog; commitlog_max_compression_buffers_in_pool=3; commitlog_periodic_queue_size=-1; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_batch_window_in_ms=NaN; commitlog_sync_period_in_ms=10000; commitlog_total_space_in_mb=null; compaction_large_partition_warning_threshold_mb=100; compaction_throughput_mb_per_sec=16; concurrent_compactors=null; concurrent_counter_writes=32; concurrent_materialized_view_writes=32; concurrent_reads=32; concurrent_replicates=null; concurrent_writes=32; continuous_paging=org.apache.cassandra.config.ContinuousPagingConfig@9caca531; counter_cache_keys_to_save=2147483647; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; credentials_cache_max_entries=1000; credentials_update_interval_in_ms=-1; credentials_validity_in_ms=2000; cross_node_timeout=false; data_file_directories=[Ljava.lang.String;@acb0951; disk_access_mode=auto; disk_failure_policy=stop; disk_optimization_estimate_percentile=0.95; disk_optimization_page_cross_chance=0.1; disk_optimization_strategy=ssd; dynamic_snitch=true; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; enable_scripted_user_defined_functions=false; enable_user_defined_functions=false; enable_user_defined_functions_threads=true; encryption_options=null; endpoint_snitch=com.datastax.bdp.snitch.DseSimpleSnitch; file_cache_size_in_mb=null; gc_log_threshold_in_ms=200; gc_warn_threshold_in_ms=1000; hinted_handoff_disabled_datacenters=[]; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; hints_compression=null; hints_directory=/var/lib/cassandra/hints; hints_flush_period_in_ms=10000; incremental_backups=false; index_interval=null; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=null; inter_dc_stream_throughput_outbound_megabits_per_sec=200; inter_dc_tcp_nodelay=false; internode_authenticator=null; internode_compression=dc; internode_recv_buff_size_in_bytes=0; internode_send_buff_size_in_bytes=0; key_cache_keys_to_save=2147483647; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=localhost; listen_interface=null; listen_interface_prefer_ipv6=false; listen_on_broadcast_address=false; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; max_hints_file_size_in_mb=128; max_mutation_size_in_kb=null; max_streaming_retries=3; max_value_size_in_mb=256; memtable_allocation_type=heap_buffers; memtable_cleanup_threshold=null; memtable_flush_writers=0; memtable_heap_space_in_mb=null; memtable_offheap_space_in_mb=null; min_free_space_per_drive_in_mb=50; native_transport_max_concurrent_connections=-1; native_transport_max_concurrent_connections_per_ip=-1; native_transport_max_frame_size_in_mb=256; native_transport_max_threads=128; native_transport_port=9042; native_transport_port_ssl=null; num_tokens=1; otc_backlog_expiration_interval_ms=200; otc_coalescing_enough_coalesced_messages=8; otc_coalescing_strategy=DISABLED; otc_coalescing_window_us=200; partitioner=org.apache.cassandra.dht.Murmur3Partitioner; permissions_cache_max_entries=1000; permissions_update_interval_in_ms=-1; permissions_validity_in_ms=2000; phi_convict_threshold=8.0; prepared_statements_cache_size_mb=null; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=5000; request_scheduler=org.apache.cassandra.scheduler.NoScheduler; request_scheduler_id=null; request_scheduler_options=null; request_timeout_in_ms=10000; role_manager=com.datastax.bdp.cassandra.auth.DseRoleManager; roles_cache_max_entries=1000; roles_update_interval_in_ms=-1; roles_validity_in_ms=2000; row_cache_class_name=org.apache.cassandra.cache.OHCProvider; row_cache_keys_to_save=2147483647; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=localhost; rpc_interface=null; rpc_interface_prefer_ipv6=false; rpc_keepalive=true; rpc_listen_backlog=50; rpc_max_threads=2147483647; rpc_min_threads=16; rpc_port=9160; rpc_recv_buff_size_in_bytes=null; rpc_send_buff_size_in_bytes=null; rpc_server_type=sync; saved_caches_directory=/var/lib/cassandra/saved_caches; seed_provider=org.apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1}; server_encryption_options=<REDACTED>; slow_query_log_timeout_in_ms=500; snapshot_before_compaction=false; ssl_storage_port=7001; sstable_preemptive_open_interval_in_mb=50; start_native_transport=true; start_rpc=true; storage_port=7000; stream_throughput_outbound_megabits_per_sec=200; streaming_keep_alive_period_in_secs=300; streaming_socket_timeout_in_ms=86400000; thrift_framed_transport_size_in_mb=15; thrift_max_message_length_in_mb=16; thrift_prepared_statements_cache_size_mb=null; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; tracetype_query_ttl=86400; tracetype_repair_ttl=604800; transparent_data_encryption_options=org.apache.cassandra.config.TransparentDataEncryptionOptions@5bf22f18; trickle_fsync=true; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=60000; unlogged_batch_across_partitions_warn_threshold=10; user_defined_function_fail_timeout=1500; user_defined_function_warn_timeout=500; user_function_timeout_policy=die; windows_timer_interval=1; write_request_timeout_in_ms=2000]
DEBUG [main] 2017-06-12 12:42:37,078  DatabaseDescriptor.java:358 - Syncing log with a period of 10000
INFO  [main] 2017-06-12 12:42:37,078  DatabaseDescriptor.java:366 - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  [main] 2017-06-12 12:42:37,079  DatabaseDescriptor.java:420 - Global memtable on-heap threshold is enabled at 455MB
INFO  [main] 2017-06-12 12:42:37,079  DatabaseDescriptor.java:424 - Global memtable off-heap threshold is enabled at 455MB
INFO  [main] 2017-06-12 12:42:37,210  RateBasedBackPressure.java:123 - Initialized back-pressure with high ratio: 0.9, factor: 5, flow: FAST, window size: 2000.
INFO  [main] 2017-06-12 12:42:37,211  DatabaseDescriptor.java:717 - Back-pressure is disabled with strategy org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}.
INFO  [main] 2017-06-12 12:42:37,257  DseConfigYamlLoader.java:38 - Loading settings from file:/Users/russellspitzer/repos/bdp/resources/dse/conf/dse.yaml
WARN  [main] 2017-06-12 12:42:37,257  DseConfigYamlLoader.java:43 - Incorrect configuration loader detected (default), DSE functionality may be impaired
INFO  [main] 2017-06-12 12:42:37,359  DseConfig.java:382 - Load of settings is done.
INFO  [main] 2017-06-12 12:42:37,360  DseConfig.java:402 - CQL slow log is enabled
INFO  [main] 2017-06-12 12:42:37,361  DseConfig.java:403 - CQL system info tables are not enabled
INFO  [main] 2017-06-12 12:42:37,361  DseConfig.java:404 - Resource level latency tracking is not enabled
INFO  [main] 2017-06-12 12:42:37,361  DseConfig.java:405 - Database summary stats are not enabled
INFO  [main] 2017-06-12 12:42:37,361  DseConfig.java:406 - Cluster summary stats are not enabled
INFO  [main] 2017-06-12 12:42:37,362  DseConfig.java:407 - Histogram data tables are not enabled
INFO  [main] 2017-06-12 12:42:37,362  DseConfig.java:408 - User level latency tracking is not enabled
INFO  [main] 2017-06-12 12:42:37,362  DseConfig.java:410 - Spark cluster info tables are not enabled
INFO  [main] 2017-06-12 12:42:37,363  DseConfig.java:444 - Cql solr query paging is: off
INFO  [main] 2017-06-12 12:42:37,365  DseUtil.java:503 - /proc/cpuinfo is not available, defaulting to 1 thread per CPU core...
INFO  [main] 2017-06-12 12:42:37,365  DseConfig.java:448 - This instance appears to have 1 thread per CPU core and 8 total CPU threads.
INFO  [main] 2017-06-12 12:42:37,367  DseConfig.java:465 - Server ID:34-36-3B-CC-92-D8
INFO  [main] 2017-06-12 12:42:37,370  Clock.java:46 - Using java.lang.System clock to generate timestamps.
WARN  [main] 2017-06-12 12:42:37,504  NettyUtil.java:64 - Found Netty's native epoll transport, but not running on linux-based operating system. Using NIO instead.
INFO  [main] 2017-06-12 12:42:37,925  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:42:37,926  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 12:42:38,062  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:42:38,144  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:42:38,144  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 12:42:38,193  Logging.scala:35 - Trying to setup a server socket at /10.150.0.180:62574 to verify connectivity with DSE node...
INFO  [main] 2017-06-12 12:42:38,214  Logging.scala:35 - Successfully verified DSE Node -> this application connectivity on random port (62574)
INFO  [main] 2017-06-12 12:42:38,264  Logging.scala:54 - Running Spark version 2.0.2.6
WARN  [main] 2017-06-12 12:42:38,417  NativeCodeLoader.java:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
INFO  [main] 2017-06-12 12:42:38,525  Logging.scala:54 - Changing view acls to: russellspitzer
INFO  [main] 2017-06-12 12:42:38,526  Logging.scala:54 - Changing modify acls to: russellspitzer
INFO  [main] 2017-06-12 12:42:38,526  Logging.scala:54 - Changing view acls groups to: 
INFO  [main] 2017-06-12 12:42:38,527  Logging.scala:54 - Changing modify acls groups to: 
INFO  [main] 2017-06-12 12:42:38,527  Logging.scala:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(russellspitzer); groups with view permissions: Set(); users  with modify permissions: Set(russellspitzer); groups with modify permissions: Set()
INFO  [main] 2017-06-12 12:42:38,589  Logging.scala:54 - Successfully started service 'sparkDriver' on port 51635.
INFO  [main] 2017-06-12 12:42:38,604  Logging.scala:54 - Registering MapOutputTracker
INFO  [main] 2017-06-12 12:42:38,619  Logging.scala:54 - Registering BlockManagerMaster
INFO  [main] 2017-06-12 12:42:38,632  Logging.scala:54 - Created local directory at /private/var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/blockmgr-d249b313-f112-428f-a0c3-365434098c96
INFO  [main] 2017-06-12 12:42:38,648  Logging.scala:54 - MemoryStore started with capacity 912.3 MB
INFO  [main] 2017-06-12 12:42:38,699  Logging.scala:54 - Registering OutputCommitCoordinator
INFO  [main] 2017-06-12 12:42:38,790  Log.java:186 - Logging initialized @3903ms
INFO  [main] 2017-06-12 12:42:38,887  Server.java:327 - jetty-9.2.z-SNAPSHOT
INFO  [main] 2017-06-12 12:42:38,914  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@c3edf4c{/jobs,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,915  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5af8bb51{/jobs/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,915  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@799ed4e8{/jobs/job,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,916  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@2e66bc32{/jobs/job/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,917  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@54d8c20d{/stages,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,918  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4b65d9f4{/stages/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,920  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@44536de4{/stages/stage,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,921  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5fcfde70{/stages/stage/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,922  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4d95a72e{/stages/pool,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,923  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@28da7d11{/stages/pool/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,924  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@77b919a3{/storage,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,924  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5624657a{/storage/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,925  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@36681447{/storage/rdd,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,925  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4d21c56e{/storage/rdd/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,926  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@726aa968{/environment,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,926  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@7100dea{/environment/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,927  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@712cfb63{/executors,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,929  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@32e54a9d{/executors/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,930  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@15639440{/executors/threadDump,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,931  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@121bb45b{/executors/threadDump/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,943  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4faa298{/static,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,943  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@1cd3b138{/,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,945  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@151bf776{/api,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,945  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5a6d30e2{/stages/stage/kill,null,AVAILABLE}
INFO  [main] 2017-06-12 12:42:38,954  AbstractConnector.java:266 - Started ServerConnector@1048a41b{HTTP/1.1}{0.0.0.0:4040}
INFO  [main] 2017-06-12 12:42:38,956  Server.java:379 - Started @4069ms
INFO  [main] 2017-06-12 12:42:38,958  Logging.scala:54 - Successfully started service 'SparkUI' on port 4040.
INFO  [main] 2017-06-12 12:42:38,960  Logging.scala:54 - Bound SparkUI to 0.0.0.0, and started at http://10.150.0.180:4040
INFO  [main] 2017-06-12 12:42:39,041  DseClusterManager.scala:28 - Creating task scheduler: 10.150.0.180:51635
INFO  [main] 2017-06-12 12:42:39,050  DseClusterManager.scala:33 - Creating scheduler backend
INFO  [main] 2017-06-12 12:42:39,059  DseClusterManager.scala:38 - Initialization
INFO  [main] 2017-06-12 12:42:39,089  Logging.scala:54 - Using connector configuration: CassandraConnectorConf(Set(/127.0.0.1),9042,NoAuthConf,None,11000,1000,4000,None,,10,5000,120000,com.datastax.spark.connector.cql.DefaultConnectionFactory$@97e93f1,CassandraSSLConf(false,None,None,JKS,TLS,Set(TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA),false,None,None,JKS))
INFO  [dse-app-client-thread-pool-1] 2017-06-12 12:42:39,094  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [dse-app-client-thread-pool-1] 2017-06-12 12:42:39,133  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [dse-app-client-thread-pool-1] 2017-06-12 12:42:39,133  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
WARN  [cluster3-nio-worker-2] 2017-06-12 12:42:39,154  RequestHandler.java:568 - /127.0.0.1:9042 replied with server error (Failed to execute method DseResourceManager.registerApplication), defuncting connection.
WARN  [cluster3-nio-worker-3] 2017-06-12 12:42:44,180  RequestHandler.java:568 - /127.0.0.1:9042 replied with server error (Failed to execute method DseResourceManager.registerApplication), defuncting connection.
INFO  [pool-1-thread-1] 2017-06-12 12:42:45,497  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
WARN  [cluster3-nio-worker-4] 2017-06-12 12:42:49,196  RequestHandler.java:568 - /127.0.0.1:9042 replied with server error (Failed to execute method DseResourceManager.registerApplication), defuncting connection.
INFO  [pool-1-thread-1] 2017-06-12 12:42:51,280  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
ERROR [dse-app-client-thread-pool-2] 2017-06-12 12:42:54,209  Logging.scala:91 - Failed to connect to DSE resource manager
java.io.IOException: Failed to register with master: dse://?
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint.org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster(DseAppClient.scala:98) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster$1.apply$mcV$sp(DseAppClient.scala:93) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster$1.apply(DseAppClient.scala:93) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster$1.apply(DseAppClient.scala:93) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[na:na]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[na:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_60]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_60]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]
Caused by: com.datastax.driver.core.exceptions.ServerError: An unexpected error occurred server side on /127.0.0.1:9042: Failed to execute method DseResourceManager.registerApplication
	at com.datastax.driver.core.exceptions.ServerError.copy(ServerError.java:54) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.exceptions.ServerError.copy(ServerError.java:16) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:28) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:236) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:59) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:42) ~[dse-java-driver-core-1.2.2.jar:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_60]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_60]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_60]
	at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_60]
	at com.datastax.spark.connector.cql.SessionProxy.invoke(SessionProxy.scala:40) ~[spark-cassandra-connector-unshaded_2.11-2.0.2.jar:2.0.2]
	at com.sun.proxy.$Proxy7.execute(Unknown Source) ~[na:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_60]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_60]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_60]
	at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_60]
	at com.datastax.spark.connector.cql.SessionProxy.invoke(SessionProxy.scala:40) ~[spark-cassandra-connector-unshaded_2.11-2.0.2.jar:2.0.2]
	at com.sun.proxy.$Proxy8.execute(Unknown Source) ~[na:na]
	at com.datastax.bdp.util.rpc.RpcUtil.call(RpcUtil.java:42) ~[dse-core-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseResourceManagerRPCClient$Impl.registerApplication(DseResourceManagerRPCClient.scala:55) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster$2.apply(DseAppClient.scala:106) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster$2.apply(DseAppClient.scala:105) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:112) ~[spark-cassandra-connector-unshaded_2.11-2.0.2.jar:2.0.2]
	at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:111) ~[spark-cassandra-connector-unshaded_2.11-2.0.2.jar:2.0.2]
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:145) ~[spark-cassandra-connector-unshaded_2.11-2.0.2.jar:2.0.2]
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:111) ~[spark-cassandra-connector-unshaded_2.11-2.0.2.jar:2.0.2]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint.org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster(DseAppClient.scala:105) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$2.apply(DseAppClient.scala:91) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint$$anonfun$2.apply(DseAppClient.scala:91) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	at scala.util.Try$.apply(Try.scala:192) ~[na:na]
	at org.apache.spark.deploy.rm.DseAppClient$ClientEndpoint.org$apache$spark$deploy$rm$DseAppClient$ClientEndpoint$$registerWithMaster(DseAppClient.scala:91) ~[dse-spark-5.1.2-8221045.jar:5.1.2-8221045]
	... 8 common frames omitted
Caused by: com.datastax.driver.core.exceptions.ServerError: An unexpected error occurred server side on /127.0.0.1:9042: Failed to execute method DseResourceManager.registerApplication
	at com.datastax.driver.core.Responses$Error.asException(Responses.java:114) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:498) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1074) ~[dse-java-driver-core-1.2.2.jar:na]
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:991) ~[dse-java-driver-core-1.2.2.jar:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:267) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) ~[netty-all-4.0.42.Final.jar:4.0.42.Final]
	... 1 common frames omitted
ERROR [dse-app-client-thread-pool-2] 2017-06-12 12:42:54,211  Logging.scala:70 - Application has been killed. Reason: Failed to connect to DSE resource manager: Failed to register with master: dse://?
WARN  [main] 2017-06-12 12:42:54,211  Logging.scala:66 - Application ID is not initialized yet.
INFO  [main] 2017-06-12 12:42:54,218  Logging.scala:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51655.
INFO  [stop-spark-context] 2017-06-12 12:42:54,218  AbstractConnector.java:306 - Stopped ServerConnector@1048a41b{HTTP/1.1}{0.0.0.0:4040}
INFO  [main] 2017-06-12 12:42:54,218  Logging.scala:54 - Server created on 10.150.0.180:51655
INFO  [stop-spark-context] 2017-06-12 12:42:54,219  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5a6d30e2{/stages/stage/kill,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,220  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@151bf776{/api,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,220  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@1cd3b138{/,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,220  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4faa298{/static,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,220  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@121bb45b{/executors/threadDump/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,221  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@15639440{/executors/threadDump,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,221  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@32e54a9d{/executors/json,null,UNAVAILABLE}
INFO  [main] 2017-06-12 12:42:54,221  Logging.scala:54 - external shuffle service port = 7437
INFO  [stop-spark-context] 2017-06-12 12:42:54,221  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@712cfb63{/executors,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,222  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@7100dea{/environment/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,222  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@726aa968{/environment,null,UNAVAILABLE}
INFO  [main] 2017-06-12 12:42:54,222  Logging.scala:54 - Registering BlockManager BlockManagerId(driver, 10.150.0.180, 51655)
INFO  [stop-spark-context] 2017-06-12 12:42:54,222  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4d21c56e{/storage/rdd/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,223  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@36681447{/storage/rdd,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,223  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5624657a{/storage/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,223  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@77b919a3{/storage,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,224  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@28da7d11{/stages/pool/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,224  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4d95a72e{/stages/pool,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,224  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5fcfde70{/stages/stage/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,224  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@44536de4{/stages/stage,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,225  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4b65d9f4{/stages/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,225  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@54d8c20d{/stages,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,225  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@2e66bc32{/jobs/job/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,226  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@799ed4e8{/jobs/job,null,UNAVAILABLE}
INFO  [dispatcher-event-loop-2] 2017-06-12 12:42:54,226  Logging.scala:54 - Registering block manager 10.150.0.180:51655 with 912.3 MB RAM, BlockManagerId(driver, 10.150.0.180, 51655)
INFO  [stop-spark-context] 2017-06-12 12:42:54,226  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5af8bb51{/jobs/json,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,226  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@c3edf4c{/jobs,null,UNAVAILABLE}
INFO  [stop-spark-context] 2017-06-12 12:42:54,227  Logging.scala:54 - Stopped Spark web UI at http://10.150.0.180:4040
INFO  [main] 2017-06-12 12:42:54,228  Logging.scala:54 - Registered BlockManager BlockManagerId(driver, 10.150.0.180, 51655)
INFO  [stop-spark-context] 2017-06-12 12:42:54,235  Logging.scala:54 - Shutting down all executors
INFO  [dispatcher-event-loop-3] 2017-06-12 12:42:54,252  Logging.scala:54 - Asking each executor to shut down
INFO  [dispatcher-event-loop-6] 2017-06-12 12:42:54,377  Logging.scala:54 - Shutting down the thread pool
INFO  [dispatcher-event-loop-6] 2017-06-12 12:42:54,378  Logging.scala:54 - Evicting connector cache
INFO  [dispatcher-event-loop-1] 2017-06-12 12:42:54,388  Logging.scala:54 - MapOutputTrackerMasterEndpoint stopped!
INFO  [stop-spark-context] 2017-06-12 12:42:54,402  Logging.scala:54 - MemoryStore cleared
INFO  [stop-spark-context] 2017-06-12 12:42:54,403  Logging.scala:54 - BlockManager stopped
INFO  [stop-spark-context] 2017-06-12 12:42:54,410  Logging.scala:54 - BlockManagerMaster stopped
INFO  [dispatcher-event-loop-7] 2017-06-12 12:42:54,414  Logging.scala:54 - OutputCommitCoordinator stopped!
ERROR [main] 2017-06-12 12:42:54,417  Logging.scala:91 - Error initializing SparkContext.
java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem
	at scala.Predef$.require(Predef.scala:219) ~[na:na]
	at org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:90) ~[spark-core_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:509) ~[spark-core_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2258) [spark-core_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$8.apply(SparkSession.scala:831) [spark-sql_2.11-2.0.2.6.jar:2.0.2.6]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$8.apply(SparkSession.scala:823) [spark-sql_2.11-2.0.2.6.jar:2.0.2.6]
	at scala.Option.getOrElse(Option.scala:121) [na:na]
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:823) [spark-sql_2.11-2.0.2.6.jar:2.0.2.6]
	at com.datastax.spark.example.WriteRead$.delayedEndpoint$com$datastax$spark$example$WriteRead$1(WriteRead.scala:18) [writeread_2.11-0.1.jar:0.1]
	at com.datastax.spark.example.WriteRead$delayedInit$body.apply(WriteRead.scala:11) [writeread_2.11-0.1.jar:0.1]
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34) [na:na]
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12) [na:na]
	at scala.App$$anonfun$main$1.apply(App.scala:76) [na:na]
	at scala.App$$anonfun$main$1.apply(App.scala:76) [na:na]
	at scala.collection.immutable.List.foreach(List.scala:381) [na:na]
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35) [na:na]
	at scala.App$class.main(App.scala:76) [na:na]
	at com.datastax.spark.example.WriteRead$.main(WriteRead.scala:11) [writeread_2.11-0.1.jar:0.1]
	at com.datastax.spark.example.WriteRead.main(WriteRead.scala) [writeread_2.11-0.1.jar:0.1]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_60]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_60]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_60]
	at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_60]
	at scala.reflect.internal.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:70) [na:na]
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31) [na:na]
	at scala.reflect.internal.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:101) [na:na]
	at scala.reflect.internal.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:70) [na:na]
	at scala.reflect.internal.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101) [na:na]
	at scala.tools.nsc.CommonRunner$class.run(ObjectRunner.scala:22) [na:na]
	at scala.tools.nsc.JarRunner$.run(MainGenericRunner.scala:13) [na:na]
	at scala.tools.nsc.CommonRunner$class.runAndCatch(ObjectRunner.scala:29) [na:na]
	at scala.tools.nsc.JarRunner$.runJar(MainGenericRunner.scala:25) [na:na]
	at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:69) [na:na]
	at scala.tools.nsc.MainGenericRunner.run$1(MainGenericRunner.scala:87) [na:na]
	at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:98) [na:na]
	at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:103) [na:na]
	at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala) [na:na]
INFO  [main] 2017-06-12 12:42:54,418  Logging.scala:54 - SparkContext already stopped.
INFO  [Thread-2] 2017-06-12 12:42:54,425  Logging.scala:54 - Shutdown hook called
INFO  [Thread-2] 2017-06-12 12:42:54,426  Logging.scala:54 - Deleting directory /private/var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/spark-734a5077-306f-4e91-a276-42f6f3165e70/userFiles-49cabf94-6e85-4249-82b2-ace688ce58ed
INFO  [Serial shutdown hooks thread] 2017-06-12 12:42:54,426  Logging.scala:35 - Successfully executed shutdown hook: Clearing session cache for C* connector
INFO  [Thread-2] 2017-06-12 12:42:54,426  Logging.scala:54 - Deleting directory /private/var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/spark-734a5077-306f-4e91-a276-42f6f3165e70
INFO  [main] 2017-06-12 12:52:52,049  GuavaCompatibility.java:126 - Detected Guava < 19 in the classpath, using legacy compatibility layer
INFO  [main] 2017-06-12 12:52:52,075  Native.java:106 - Could not load JNR C Library, native system calls through this library will not be available (set this logger level to DEBUG to see the full stack trace).
INFO  [main] 2017-06-12 12:52:52,076  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:52:52,189  YamlConfigurationLoader.java:89 - Configuration location: file:/Users/russellspitzer/repos/bdp/resources/cassandra/conf/cassandra.yaml
DEBUG [main] 2017-06-12 12:52:52,191  YamlConfigurationLoader.java:108 - Loading settings from file:/Users/russellspitzer/repos/bdp/resources/cassandra/conf/cassandra.yaml
INFO  [main] 2017-06-12 12:52:52,469  Config.java:487 - Node configuration:[allocate_tokens_for_keyspace=null; allocate_tokens_for_local_replication_factor=null; authenticator=com.datastax.bdp.cassandra.auth.DseAuthenticator; authorizer=com.datastax.bdp.cassandra.auth.DseAuthorizer; auto_bootstrap=true; auto_snapshot=true; back_pressure_enabled=false; back_pressure_strategy=org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}; batch_size_fail_threshold_in_kb=640; batch_size_warn_threshold_in_kb=64; batchlog_replay_throttle_in_kb=1024; broadcast_address=null; broadcast_rpc_address=null; buffer_pool_use_heap_if_exhausted=true; cas_contention_timeout_in_ms=1000; cdc_enabled=false; cdc_free_space_check_interval_ms=250; cdc_raw_directory=/var/lib/cassandra/cdc_raw; cdc_total_space_in_mb=0; client_encryption_options=<REDACTED>; cluster_name=Test Cluster; column_index_cache_size_in_kb=2; column_index_size_in_kb=64; commit_failure_policy=stop; commitlog_compression=null; commitlog_directory=/var/lib/cassandra/commitlog; commitlog_max_compression_buffers_in_pool=3; commitlog_periodic_queue_size=-1; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_batch_window_in_ms=NaN; commitlog_sync_period_in_ms=10000; commitlog_total_space_in_mb=null; compaction_large_partition_warning_threshold_mb=100; compaction_throughput_mb_per_sec=16; concurrent_compactors=null; concurrent_counter_writes=32; concurrent_materialized_view_writes=32; concurrent_reads=32; concurrent_replicates=null; concurrent_writes=32; continuous_paging=org.apache.cassandra.config.ContinuousPagingConfig@9caca531; counter_cache_keys_to_save=2147483647; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; credentials_cache_max_entries=1000; credentials_update_interval_in_ms=-1; credentials_validity_in_ms=2000; cross_node_timeout=false; data_file_directories=[Ljava.lang.String;@acb0951; disk_access_mode=auto; disk_failure_policy=stop; disk_optimization_estimate_percentile=0.95; disk_optimization_page_cross_chance=0.1; disk_optimization_strategy=ssd; dynamic_snitch=true; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; enable_scripted_user_defined_functions=false; enable_user_defined_functions=false; enable_user_defined_functions_threads=true; encryption_options=null; endpoint_snitch=com.datastax.bdp.snitch.DseSimpleSnitch; file_cache_size_in_mb=null; gc_log_threshold_in_ms=200; gc_warn_threshold_in_ms=1000; hinted_handoff_disabled_datacenters=[]; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; hints_compression=null; hints_directory=/var/lib/cassandra/hints; hints_flush_period_in_ms=10000; incremental_backups=false; index_interval=null; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=null; inter_dc_stream_throughput_outbound_megabits_per_sec=200; inter_dc_tcp_nodelay=false; internode_authenticator=null; internode_compression=dc; internode_recv_buff_size_in_bytes=0; internode_send_buff_size_in_bytes=0; key_cache_keys_to_save=2147483647; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=localhost; listen_interface=null; listen_interface_prefer_ipv6=false; listen_on_broadcast_address=false; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; max_hints_file_size_in_mb=128; max_mutation_size_in_kb=null; max_streaming_retries=3; max_value_size_in_mb=256; memtable_allocation_type=heap_buffers; memtable_cleanup_threshold=null; memtable_flush_writers=0; memtable_heap_space_in_mb=null; memtable_offheap_space_in_mb=null; min_free_space_per_drive_in_mb=50; native_transport_max_concurrent_connections=-1; native_transport_max_concurrent_connections_per_ip=-1; native_transport_max_frame_size_in_mb=256; native_transport_max_threads=128; native_transport_port=9042; native_transport_port_ssl=null; num_tokens=1; otc_backlog_expiration_interval_ms=200; otc_coalescing_enough_coalesced_messages=8; otc_coalescing_strategy=DISABLED; otc_coalescing_window_us=200; partitioner=org.apache.cassandra.dht.Murmur3Partitioner; permissions_cache_max_entries=1000; permissions_update_interval_in_ms=-1; permissions_validity_in_ms=2000; phi_convict_threshold=8.0; prepared_statements_cache_size_mb=null; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=5000; request_scheduler=org.apache.cassandra.scheduler.NoScheduler; request_scheduler_id=null; request_scheduler_options=null; request_timeout_in_ms=10000; role_manager=com.datastax.bdp.cassandra.auth.DseRoleManager; roles_cache_max_entries=1000; roles_update_interval_in_ms=-1; roles_validity_in_ms=2000; row_cache_class_name=org.apache.cassandra.cache.OHCProvider; row_cache_keys_to_save=2147483647; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=localhost; rpc_interface=null; rpc_interface_prefer_ipv6=false; rpc_keepalive=true; rpc_listen_backlog=50; rpc_max_threads=2147483647; rpc_min_threads=16; rpc_port=9160; rpc_recv_buff_size_in_bytes=null; rpc_send_buff_size_in_bytes=null; rpc_server_type=sync; saved_caches_directory=/var/lib/cassandra/saved_caches; seed_provider=org.apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1}; server_encryption_options=<REDACTED>; slow_query_log_timeout_in_ms=500; snapshot_before_compaction=false; ssl_storage_port=7001; sstable_preemptive_open_interval_in_mb=50; start_native_transport=true; start_rpc=true; storage_port=7000; stream_throughput_outbound_megabits_per_sec=200; streaming_keep_alive_period_in_secs=300; streaming_socket_timeout_in_ms=86400000; thrift_framed_transport_size_in_mb=15; thrift_max_message_length_in_mb=16; thrift_prepared_statements_cache_size_mb=null; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; tracetype_query_ttl=86400; tracetype_repair_ttl=604800; transparent_data_encryption_options=org.apache.cassandra.config.TransparentDataEncryptionOptions@5bf22f18; trickle_fsync=true; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=60000; unlogged_batch_across_partitions_warn_threshold=10; user_defined_function_fail_timeout=1500; user_defined_function_warn_timeout=500; user_function_timeout_policy=die; windows_timer_interval=1; write_request_timeout_in_ms=2000]
DEBUG [main] 2017-06-12 12:52:52,470  DatabaseDescriptor.java:358 - Syncing log with a period of 10000
INFO  [main] 2017-06-12 12:52:52,470  DatabaseDescriptor.java:366 - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  [main] 2017-06-12 12:52:52,471  DatabaseDescriptor.java:420 - Global memtable on-heap threshold is enabled at 455MB
INFO  [main] 2017-06-12 12:52:52,471  DatabaseDescriptor.java:424 - Global memtable off-heap threshold is enabled at 455MB
INFO  [main] 2017-06-12 12:52:52,632  RateBasedBackPressure.java:123 - Initialized back-pressure with high ratio: 0.9, factor: 5, flow: FAST, window size: 2000.
INFO  [main] 2017-06-12 12:52:52,633  DatabaseDescriptor.java:717 - Back-pressure is disabled with strategy org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}.
INFO  [main] 2017-06-12 12:52:52,680  DseConfigYamlLoader.java:38 - Loading settings from file:/Users/russellspitzer/repos/bdp/resources/dse/conf/dse.yaml
WARN  [main] 2017-06-12 12:52:52,681  DseConfigYamlLoader.java:43 - Incorrect configuration loader detected (default), DSE functionality may be impaired
INFO  [main] 2017-06-12 12:52:52,782  DseConfig.java:382 - Load of settings is done.
INFO  [main] 2017-06-12 12:52:52,784  DseConfig.java:402 - CQL slow log is enabled
INFO  [main] 2017-06-12 12:52:52,784  DseConfig.java:403 - CQL system info tables are not enabled
INFO  [main] 2017-06-12 12:52:52,785  DseConfig.java:404 - Resource level latency tracking is not enabled
INFO  [main] 2017-06-12 12:52:52,785  DseConfig.java:405 - Database summary stats are not enabled
INFO  [main] 2017-06-12 12:52:52,785  DseConfig.java:406 - Cluster summary stats are not enabled
INFO  [main] 2017-06-12 12:52:52,786  DseConfig.java:407 - Histogram data tables are not enabled
INFO  [main] 2017-06-12 12:52:52,786  DseConfig.java:408 - User level latency tracking is not enabled
INFO  [main] 2017-06-12 12:52:52,786  DseConfig.java:410 - Spark cluster info tables are not enabled
INFO  [main] 2017-06-12 12:52:52,787  DseConfig.java:444 - Cql solr query paging is: off
INFO  [main] 2017-06-12 12:52:52,789  DseUtil.java:503 - /proc/cpuinfo is not available, defaulting to 1 thread per CPU core...
INFO  [main] 2017-06-12 12:52:52,790  DseConfig.java:448 - This instance appears to have 1 thread per CPU core and 8 total CPU threads.
INFO  [main] 2017-06-12 12:52:52,791  DseConfig.java:465 - Server ID:34-36-3B-CC-92-D8
INFO  [main] 2017-06-12 12:52:52,795  Clock.java:46 - Using java.lang.System clock to generate timestamps.
WARN  [main] 2017-06-12 12:52:52,929  NettyUtil.java:64 - Found Netty's native epoll transport, but not running on linux-based operating system. Using NIO instead.
INFO  [main] 2017-06-12 12:52:53,350  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:52:53,352  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 12:52:53,486  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:52:53,544  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:52:53,544  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 12:52:53,565  Logging.scala:35 - Trying to setup a server socket at /10.150.0.180:35772 to verify connectivity with DSE node...
INFO  [main] 2017-06-12 12:52:53,585  Logging.scala:35 - Successfully verified DSE Node -> this application connectivity on random port (35772)
INFO  [main] 2017-06-12 12:52:53,627  Logging.scala:54 - Running Spark version 2.0.2.6
WARN  [main] 2017-06-12 12:52:53,747  NativeCodeLoader.java:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
INFO  [main] 2017-06-12 12:52:53,833  Logging.scala:54 - Changing view acls to: russellspitzer
INFO  [main] 2017-06-12 12:52:53,834  Logging.scala:54 - Changing modify acls to: russellspitzer
INFO  [main] 2017-06-12 12:52:53,835  Logging.scala:54 - Changing view acls groups to: 
INFO  [main] 2017-06-12 12:52:53,836  Logging.scala:54 - Changing modify acls groups to: 
INFO  [main] 2017-06-12 12:52:53,837  Logging.scala:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(russellspitzer); groups with view permissions: Set(); users  with modify permissions: Set(russellspitzer); groups with modify permissions: Set()
INFO  [main] 2017-06-12 12:52:53,897  Logging.scala:54 - Successfully started service 'sparkDriver' on port 51796.
INFO  [main] 2017-06-12 12:52:53,921  Logging.scala:54 - Registering MapOutputTracker
INFO  [main] 2017-06-12 12:52:53,943  Logging.scala:54 - Registering BlockManagerMaster
INFO  [main] 2017-06-12 12:52:53,958  Logging.scala:54 - Created local directory at /private/var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/blockmgr-2314dc50-12a0-4fb9-873c-5a47fd9a88fd
INFO  [main] 2017-06-12 12:52:53,975  Logging.scala:54 - MemoryStore started with capacity 912.3 MB
INFO  [main] 2017-06-12 12:52:54,020  Logging.scala:54 - Registering OutputCommitCoordinator
INFO  [main] 2017-06-12 12:52:54,102  Log.java:186 - Logging initialized @3821ms
INFO  [main] 2017-06-12 12:52:54,182  Server.java:327 - jetty-9.2.z-SNAPSHOT
INFO  [main] 2017-06-12 12:52:54,199  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5f0f9947{/jobs,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,200  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@1aad0b1{/jobs/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,200  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@c3edf4c{/jobs/job,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,201  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5af8bb51{/jobs/job/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,201  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@799ed4e8{/stages,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,202  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@2e66bc32{/stages/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,202  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@54d8c20d{/stages/stage,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,203  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4b65d9f4{/stages/stage/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,203  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@44536de4{/stages/pool,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,204  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5fcfde70{/stages/pool/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,204  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4d95a72e{/storage,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,205  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@28da7d11{/storage/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,205  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@77b919a3{/storage/rdd,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,205  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5624657a{/storage/rdd/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,206  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@36681447{/environment,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,206  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4d21c56e{/environment/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,207  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@726aa968{/executors,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,207  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@7100dea{/executors/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,208  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@712cfb63{/executors/threadDump,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,208  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@32e54a9d{/executors/threadDump/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,213  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@15639440{/static,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,213  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@121bb45b{/,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,214  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4faa298{/api,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,215  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@1cd3b138{/stages/stage/kill,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,222  AbstractConnector.java:266 - Started ServerConnector@70972170{HTTP/1.1}{0.0.0.0:4040}
INFO  [main] 2017-06-12 12:52:54,222  Server.java:379 - Started @3941ms
INFO  [main] 2017-06-12 12:52:54,223  Logging.scala:54 - Successfully started service 'SparkUI' on port 4040.
INFO  [main] 2017-06-12 12:52:54,225  Logging.scala:54 - Bound SparkUI to 0.0.0.0, and started at http://10.150.0.180:4040
INFO  [main] 2017-06-12 12:52:54,277  DseClusterManager.scala:28 - Creating task scheduler: 10.150.0.180:51796
INFO  [main] 2017-06-12 12:52:54,290  DseClusterManager.scala:33 - Creating scheduler backend
INFO  [main] 2017-06-12 12:52:54,302  DseClusterManager.scala:38 - Initialization
INFO  [main] 2017-06-12 12:52:54,341  Logging.scala:54 - Using connector configuration: CassandraConnectorConf(Set(/127.0.0.1),9042,NoAuthConf,None,11000,1000,4000,None,,10,5000,120000,com.datastax.spark.connector.cql.DefaultConnectionFactory$@97e93f1,CassandraSSLConf(false,None,None,JKS,TLS,Set(TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA),false,None,None,JKS))
INFO  [dse-app-client-thread-pool-1] 2017-06-12 12:52:54,345  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [dse-app-client-thread-pool-1] 2017-06-12 12:52:54,400  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [dse-app-client-thread-pool-1] 2017-06-12 12:52:54,400  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [dse-app-client-thread-pool-2] 2017-06-12 12:52:54,489  Logging.scala:54 - Connected to Spark cluster with app ID app-20170612125254-0000
INFO  [main] 2017-06-12 12:52:54,497  Logging.scala:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51799.
INFO  [main] 2017-06-12 12:52:54,498  Logging.scala:54 - Server created on 10.150.0.180:51799
INFO  [main] 2017-06-12 12:52:54,504  Logging.scala:54 - external shuffle service port = 7437
INFO  [main] 2017-06-12 12:52:54,505  Logging.scala:54 - Registering BlockManager BlockManagerId(driver, 10.150.0.180, 51799)
INFO  [dispatcher-event-loop-3] 2017-06-12 12:52:54,509  Logging.scala:54 - Registering block manager 10.150.0.180:51799 with 912.3 MB RAM, BlockManagerId(driver, 10.150.0.180, 51799)
INFO  [main] 2017-06-12 12:52:54,514  Logging.scala:54 - Registered BlockManager BlockManagerId(driver, 10.150.0.180, 51799)
INFO  [main] 2017-06-12 12:52:54,723  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@7af3874e{/metrics/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,741  Logging.scala:54 - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
WARN  [main] 2017-06-12 12:52:54,746  Logging.scala:66 - Use an existing SparkContext, some configuration may not take effect.
INFO  [main] 2017-06-12 12:52:54,776  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@2b5c4f17{/SQL,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,777  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@69f0b0f4{/SQL/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,778  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@177c41d7{/SQL/execution,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,779  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@1d61a348{/SQL/execution/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,781  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@8cc8cdb{/static/sql,null,AVAILABLE}
INFO  [main] 2017-06-12 12:52:54,829  Logging.scala:54 - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('dsefs:///user/spark/warehouse').
INFO  [main] 2017-06-12 12:52:54,838  Logging.scala:54 - Warehouse path is 'dsefs:/user/spark/warehouse'.
INFO  [main] 2017-06-12 12:52:56,592  Logging.scala:54 - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
INFO  [main] 2017-06-12 12:52:57,762  HiveMetaStore.java:589 - 0: Opening raw store with implemenation class:com.datastax.bdp.hadoop.hive.metastore.CassandraHiveMetaStore
INFO  [dispatcher-event-loop-1] 2017-06-12 12:52:57,880  Logging.scala:54 - Registered executor NettyRpcEndpointRef(null) (10.150.0.180:51802) with ID 0
INFO  [main] 2017-06-12 12:52:57,910  GuavaCompatibility.java:126 - Detected Guava < 19 in the classpath, using legacy compatibility layer
INFO  [dispatcher-event-loop-7] 2017-06-12 12:52:57,957  Logging.scala:54 - Registering block manager 127.0.0.1:51803 with 366.3 MB RAM, BlockManagerId(0, 127.0.0.1, 51803)
INFO  [main] 2017-06-12 12:52:57,962  DseConnectionUtil.java:175 - Using no credentials
INFO  [main] 2017-06-12 12:52:57,977  Native.java:106 - Could not load JNR C Library, native system calls through this library will not be available (set this logger level to DEBUG to see the full stack trace).
INFO  [main] 2017-06-12 12:52:57,978  Clock.java:46 - Using java.lang.System clock to generate timestamps.
WARN  [main] 2017-06-12 12:52:58,203  NettyUtil.java:64 - Found Netty's native epoll transport, but not running on linux-based operating system. Using NIO instead.
INFO  [main] 2017-06-12 12:52:58,825  DCAwareRoundRobinPolicy.java:86 - Using data-center name 'Analytics' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)
INFO  [main] 2017-06-12 12:52:58,827  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:52:58,875  SchemaManagerService.java:89 - Refresh cluster meta data
INFO  [main] 2017-06-12 12:52:58,876  DseConnectionUtil.java:175 - Using no credentials
INFO  [main] 2017-06-12 12:52:58,876  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:52:58,927  DCAwareRoundRobinPolicy.java:86 - Using data-center name 'Analytics' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)
INFO  [main] 2017-06-12 12:52:58,927  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:52:59,137  DseRestClientAuthProviderBuilderFactory.scala:35 - Using no authentication for DSEFS
INFO  [main] 2017-06-12 12:52:59,486  HiveMetaStore.java:663 - Added admin role in metastore
INFO  [main] 2017-06-12 12:52:59,489  HiveMetaStore.java:672 - Added public role in metastore
INFO  [main] 2017-06-12 12:52:59,502  HiveMetaStore.java:712 - No user is added in admin role, since config is empty
INFO  [main] 2017-06-12 12:52:59,627  HiveMetaStore.java:746 - 0: get_all_databases
INFO  [main] 2017-06-12 12:52:59,629  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=get_all_databases	
INFO  [main] 2017-06-12 12:52:59,632  SchemaManagerService.java:219 - Updating Cassandra Keyspace to Metastore Database Mapping
INFO  [main] 2017-06-12 12:52:59,632  SchemaManagerService.java:89 - Refresh cluster meta data
INFO  [main] 2017-06-12 12:52:59,633  DseConnectionUtil.java:175 - Using no credentials
INFO  [main] 2017-06-12 12:52:59,634  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:52:59,698  DCAwareRoundRobinPolicy.java:86 - Using data-center name 'Analytics' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)
INFO  [main] 2017-06-12 12:52:59,700  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:52:59,704  SchemaManagerService.java:89 - Refresh cluster meta data
INFO  [main] 2017-06-12 12:52:59,706  DseConnectionUtil.java:175 - Using no credentials
INFO  [main] 2017-06-12 12:52:59,706  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:52:59,743  DCAwareRoundRobinPolicy.java:86 - Using data-center name 'Analytics' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)
INFO  [main] 2017-06-12 12:52:59,743  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:52:59,763  SchemaManagerService.java:89 - Refresh cluster meta data
INFO  [main] 2017-06-12 12:52:59,765  DseConnectionUtil.java:175 - Using no credentials
INFO  [main] 2017-06-12 12:52:59,766  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:52:59,803  DCAwareRoundRobinPolicy.java:86 - Using data-center name 'Analytics' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)
INFO  [main] 2017-06-12 12:52:59,803  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:52:59,821  HiveMetaStore.java:746 - 0: get_functions: db=default pat=*
INFO  [main] 2017-06-12 12:52:59,821  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
INFO  [main] 2017-06-12 12:52:59,822  CassandraHiveMetaStore.java:1515 - in getFunctions with dbName: default and functionNamePattern: *
INFO  [main] 2017-06-12 12:52:59,824  HiveMetaStore.java:746 - 0: get_functions: db=dsefs pat=*
INFO  [main] 2017-06-12 12:52:59,825  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=get_functions: db=dsefs pat=*	
INFO  [main] 2017-06-12 12:52:59,825  CassandraHiveMetaStore.java:1515 - in getFunctions with dbName: dsefs and functionNamePattern: *
INFO  [main] 2017-06-12 12:52:59,827  HiveMetaStore.java:746 - 0: get_functions: db=ks pat=*
INFO  [main] 2017-06-12 12:52:59,828  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=get_functions: db=ks pat=*	
INFO  [main] 2017-06-12 12:52:59,828  CassandraHiveMetaStore.java:1515 - in getFunctions with dbName: ks and functionNamePattern: *
INFO  [main] 2017-06-12 12:53:00,109  DseRestClientAuthProviderBuilderFactory.scala:35 - Using no authentication for DSEFS
INFO  [main] 2017-06-12 12:53:00,544  SessionState.java:641 - Created HDFS directory: /tmp/hive/russellspitzer
INFO  [main] 2017-06-12 12:53:00,572  SessionState.java:641 - Created local directory: /var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/d51966cf-dc91-412b-99ec-924d228154f3_resources
INFO  [main] 2017-06-12 12:53:00,644  SessionState.java:641 - Created HDFS directory: /tmp/hive/russellspitzer/d51966cf-dc91-412b-99ec-924d228154f3
INFO  [main] 2017-06-12 12:53:00,695  SessionState.java:641 - Created local directory: /var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/russellspitzer/d51966cf-dc91-412b-99ec-924d228154f3
INFO  [main] 2017-06-12 12:53:00,775  SessionState.java:641 - Created HDFS directory: /tmp/hive/russellspitzer/d51966cf-dc91-412b-99ec-924d228154f3/_tmp_space.db
INFO  [main] 2017-06-12 12:53:00,807  Logging.scala:54 - Warehouse location for Hive client (version 1.2.1) is dsefs:///user/spark/warehouse
INFO  [main] 2017-06-12 12:53:01,027  SessionState.java:641 - Created local directory: /var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/be7e4c3e-1b34-4240-8d31-794c87445b8c_resources
INFO  [main] 2017-06-12 12:53:01,105  SessionState.java:641 - Created HDFS directory: /tmp/hive/russellspitzer/be7e4c3e-1b34-4240-8d31-794c87445b8c
INFO  [main] 2017-06-12 12:53:01,152  SessionState.java:641 - Created local directory: /var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/russellspitzer/be7e4c3e-1b34-4240-8d31-794c87445b8c
INFO  [main] 2017-06-12 12:53:01,230  SessionState.java:641 - Created HDFS directory: /tmp/hive/russellspitzer/be7e4c3e-1b34-4240-8d31-794c87445b8c/_tmp_space.db
INFO  [main] 2017-06-12 12:53:01,259  Logging.scala:54 - Warehouse location for Hive client (version 1.2.1) is dsefs:///user/spark/warehouse
INFO  [main] 2017-06-12 12:53:01,579  HiveMetaStore.java:746 - 0: create_database: Database(name:default, description:default database, locationUri:dsefs://127.0.0.1/user/spark/warehouse, parameters:{})
INFO  [main] 2017-06-12 12:53:01,579  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:dsefs://127.0.0.1/user/spark/warehouse, parameters:{})	
ERROR [main] 2017-06-12 12:53:01,587  RetryingHMSHandler.java:159 - AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy17.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy18.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:280)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:269)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:308)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:51)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:161)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)
	at org.apache.spark.sql.SparkSession.range(SparkSession.scala:509)
	at org.apache.spark.sql.SparkSession.range(SparkSession.scala:484)
	at com.datastax.spark.example.WriteRead$.delayedEndpoint$com$datastax$spark$example$WriteRead$1(WriteRead.scala:31)
	at com.datastax.spark.example.WriteRead$delayedInit$body.apply(WriteRead.scala:11)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.App$$anonfun$main$1.apply(App.scala:76)
	at scala.App$$anonfun$main$1.apply(App.scala:76)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
	at scala.App$class.main(App.scala:76)
	at com.datastax.spark.example.WriteRead$.main(WriteRead.scala:11)
	at com.datastax.spark.example.WriteRead.main(WriteRead.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at scala.reflect.internal.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:70)
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
	at scala.reflect.internal.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:101)
	at scala.reflect.internal.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:70)
	at scala.reflect.internal.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101)
	at scala.tools.nsc.CommonRunner$class.run(ObjectRunner.scala:22)
	at scala.tools.nsc.JarRunner$.run(MainGenericRunner.scala:13)
	at scala.tools.nsc.CommonRunner$class.runAndCatch(ObjectRunner.scala:29)
	at scala.tools.nsc.JarRunner$.runJar(MainGenericRunner.scala:25)
	at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:69)
	at scala.tools.nsc.MainGenericRunner.run$1(MainGenericRunner.scala:87)
	at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:98)
	at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:103)
	at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala)

INFO  [pool-1-thread-1] 2017-06-12 12:53:02,513  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 12:53:02,575  Logging.scala:54 - Code generated in 54.468973 ms
INFO  [main] 2017-06-12 12:53:02,721  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:53:02,770  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:53:02,770  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 12:53:03,064  Logging.scala:54 - Starting job: runJob at RDDFunctions.scala:36
INFO  [dag-scheduler-event-loop] 2017-06-12 12:53:03,079  Logging.scala:54 - Got job 0 (runJob at RDDFunctions.scala:36) with 2 output partitions
INFO  [dag-scheduler-event-loop] 2017-06-12 12:53:03,080  Logging.scala:54 - Final stage: ResultStage 0 (runJob at RDDFunctions.scala:36)
INFO  [dag-scheduler-event-loop] 2017-06-12 12:53:03,080  Logging.scala:54 - Parents of final stage: List()
INFO  [dag-scheduler-event-loop] 2017-06-12 12:53:03,082  Logging.scala:54 - Missing parents: List()
INFO  [dag-scheduler-event-loop] 2017-06-12 12:53:03,094  Logging.scala:54 - Submitting ResultStage 0 (MapPartitionsRDD[5] at rdd at WriteRead.scala:33), which has no missing parents
INFO  [dag-scheduler-event-loop] 2017-06-12 12:53:03,208  Logging.scala:54 - Block broadcast_0 stored as values in memory (estimated size 18.0 KB, free 912.3 MB)
INFO  [dag-scheduler-event-loop] 2017-06-12 12:53:03,241  Logging.scala:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 8.4 KB, free 912.3 MB)
INFO  [dispatcher-event-loop-7] 2017-06-12 12:53:03,243  Logging.scala:54 - Added broadcast_0_piece0 in memory on 10.150.0.180:51799 (size: 8.4 KB, free: 912.3 MB)
INFO  [dag-scheduler-event-loop] 2017-06-12 12:53:03,247  Logging.scala:54 - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
INFO  [dag-scheduler-event-loop] 2017-06-12 12:53:03,254  Logging.scala:54 - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at rdd at WriteRead.scala:33)
INFO  [dag-scheduler-event-loop] 2017-06-12 12:53:03,257  Logging.scala:54 - Adding task set 0.0 with 2 tasks
INFO  [dispatcher-event-loop-0] 2017-06-12 12:53:03,292  Logging.scala:54 - Starting task 0.0 in stage 0.0 (TID 0, 127.0.0.1, partition 0, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-0] 2017-06-12 12:53:03,297  Logging.scala:54 - Starting task 1.0 in stage 0.0 (TID 1, 127.0.0.1, partition 1, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-0] 2017-06-12 12:53:03,305  Logging.scala:54 - Launching task 0 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-0] 2017-06-12 12:53:03,307  Logging.scala:54 - Launching task 1 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-4] 2017-06-12 12:53:03,533  Logging.scala:54 - Added broadcast_0_piece0 in memory on 127.0.0.1:51803 (size: 8.4 KB, free: 366.3 MB)
WARN  [task-result-getter-0] 2017-06-12 12:53:04,030  Logging.scala:66 - Lost task 1.0 in stage 0.0 (TID 1, 127.0.0.1): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2089)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1261)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2006)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

INFO  [task-result-getter-1] 2017-06-12 12:53:04,034  Logging.scala:54 - Lost task 0.0 in stage 0.0 (TID 0) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 1]
INFO  [dispatcher-event-loop-0] 2017-06-12 12:53:04,036  Logging.scala:54 - Starting task 0.1 in stage 0.0 (TID 2, 127.0.0.1, partition 0, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-0] 2017-06-12 12:53:04,037  Logging.scala:54 - Starting task 1.1 in stage 0.0 (TID 3, 127.0.0.1, partition 1, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-0] 2017-06-12 12:53:04,038  Logging.scala:54 - Launching task 2 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-0] 2017-06-12 12:53:04,038  Logging.scala:54 - Launching task 3 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-2] 2017-06-12 12:53:04,058  Logging.scala:54 - Lost task 1.1 in stage 0.0 (TID 3) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 2]
INFO  [dispatcher-event-loop-4] 2017-06-12 12:53:04,060  Logging.scala:54 - Starting task 1.2 in stage 0.0 (TID 4, 127.0.0.1, partition 1, PROCESS_LOCAL, 5289 bytes)
INFO  [task-result-getter-3] 2017-06-12 12:53:04,060  Logging.scala:54 - Lost task 0.1 in stage 0.0 (TID 2) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 3]
INFO  [dispatcher-event-loop-4] 2017-06-12 12:53:04,060  Logging.scala:54 - Launching task 4 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-4] 2017-06-12 12:53:04,062  Logging.scala:54 - Starting task 0.2 in stage 0.0 (TID 5, 127.0.0.1, partition 0, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-4] 2017-06-12 12:53:04,062  Logging.scala:54 - Launching task 5 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-0] 2017-06-12 12:53:04,084  Logging.scala:54 - Lost task 0.2 in stage 0.0 (TID 5) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 4]
INFO  [dispatcher-event-loop-5] 2017-06-12 12:53:04,087  Logging.scala:54 - Starting task 0.3 in stage 0.0 (TID 6, 127.0.0.1, partition 0, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-5] 2017-06-12 12:53:04,087  Logging.scala:54 - Launching task 6 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-1] 2017-06-12 12:53:04,089  Logging.scala:54 - Lost task 1.2 in stage 0.0 (TID 4) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 5]
INFO  [dispatcher-event-loop-7] 2017-06-12 12:53:04,090  Logging.scala:54 - Starting task 1.3 in stage 0.0 (TID 7, 127.0.0.1, partition 1, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-7] 2017-06-12 12:53:04,090  Logging.scala:54 - Launching task 7 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-2] 2017-06-12 12:53:04,102  Logging.scala:54 - Lost task 0.3 in stage 0.0 (TID 6) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 6]
ERROR [task-result-getter-2] 2017-06-12 12:53:04,103  Logging.scala:70 - Task 0 in stage 0.0 failed 4 times; aborting job
INFO  [task-result-getter-3] 2017-06-12 12:53:04,105  Logging.scala:54 - Lost task 1.3 in stage 0.0 (TID 7) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 7]
INFO  [task-result-getter-3] 2017-06-12 12:53:04,107  Logging.scala:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
INFO  [dag-scheduler-event-loop] 2017-06-12 12:53:04,109  Logging.scala:54 - Cancelling stage 0
INFO  [dag-scheduler-event-loop] 2017-06-12 12:53:04,111  Logging.scala:54 - ResultStage 0 (runJob at RDDFunctions.scala:36) failed in 0.843 s
INFO  [main] 2017-06-12 12:53:04,113  Logging.scala:54 - Job 0 failed: runJob at RDDFunctions.scala:36, took 1.048660 s
INFO  [Thread-2] 2017-06-12 12:53:04,126  Logging.scala:54 - Invoking stop() from shutdown hook
INFO  [Thread-2] 2017-06-12 12:53:04,135  AbstractConnector.java:306 - Stopped ServerConnector@70972170{HTTP/1.1}{0.0.0.0:4040}
INFO  [Thread-2] 2017-06-12 12:53:04,137  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@1cd3b138{/stages/stage/kill,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,138  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4faa298{/api,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,138  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@121bb45b{/,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,138  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@15639440{/static,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,139  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@32e54a9d{/executors/threadDump/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,139  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@712cfb63{/executors/threadDump,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,139  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@7100dea{/executors/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,139  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@726aa968{/executors,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,140  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4d21c56e{/environment/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,140  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@36681447{/environment,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,140  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5624657a{/storage/rdd/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,148  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@77b919a3{/storage/rdd,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,148  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@28da7d11{/storage/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,148  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4d95a72e{/storage,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,149  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5fcfde70{/stages/pool/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,149  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@44536de4{/stages/pool,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,149  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4b65d9f4{/stages/stage/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,149  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@54d8c20d{/stages/stage,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,150  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@2e66bc32{/stages/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,150  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@799ed4e8{/stages,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,150  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5af8bb51{/jobs/job/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,150  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@c3edf4c{/jobs/job,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,151  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@1aad0b1{/jobs/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,151  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5f0f9947{/jobs,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:53:04,154  Logging.scala:54 - Stopped Spark web UI at http://10.150.0.180:4040
INFO  [Thread-2] 2017-06-12 12:53:04,180  Logging.scala:54 - Shutting down all executors
INFO  [dispatcher-event-loop-3] 2017-06-12 12:53:04,182  Logging.scala:54 - Asking each executor to shut down
INFO  [Serial shutdown hooks thread] 2017-06-12 12:53:06,338  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [Serial shutdown hooks thread] 2017-06-12 12:53:08,554  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [dispatcher-event-loop-5] 2017-06-12 12:53:09,242  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [dispatcher-event-loop-5] 2017-06-12 12:53:09,276  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [dispatcher-event-loop-5] 2017-06-12 12:53:09,276  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [Serial shutdown hooks thread] 2017-06-12 12:53:10,767  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [Serial shutdown hooks thread] 2017-06-12 12:53:10,769  Logging.scala:35 - Successfully executed shutdown hook: Clearing session cache for C* connector
INFO  [dispatcher-event-loop-5] 2017-06-12 12:53:11,539  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [dispatcher-event-loop-5] 2017-06-12 12:53:11,540  Logging.scala:54 - Shutting down the thread pool
INFO  [dispatcher-event-loop-5] 2017-06-12 12:53:11,541  Logging.scala:54 - Evicting connector cache
INFO  [dispatcher-event-loop-5] 2017-06-12 12:53:11,543  Logging.scala:54 - onStop complete
INFO  [dispatcher-event-loop-3] 2017-06-12 12:53:11,545  Logging.scala:54 - MapOutputTrackerMasterEndpoint stopped!
INFO  [Thread-2] 2017-06-12 12:53:11,551  Logging.scala:54 - MemoryStore cleared
INFO  [Thread-2] 2017-06-12 12:53:11,552  Logging.scala:54 - BlockManager stopped
INFO  [Thread-2] 2017-06-12 12:53:11,556  Logging.scala:54 - BlockManagerMaster stopped
INFO  [dispatcher-event-loop-2] 2017-06-12 12:53:11,559  Logging.scala:54 - OutputCommitCoordinator stopped!
INFO  [Thread-2] 2017-06-12 12:53:11,560  Logging.scala:54 - Successfully stopped SparkContext
INFO  [Thread-2] 2017-06-12 12:53:11,561  Logging.scala:54 - Shutdown hook called
INFO  [Thread-2] 2017-06-12 12:53:11,562  Logging.scala:54 - Deleting directory /private/var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/spark-001ba944-73fe-4002-9245-d45d76cc1974
INFO  [main] 2017-06-12 12:57:12,809  GuavaCompatibility.java:126 - Detected Guava < 19 in the classpath, using legacy compatibility layer
INFO  [main] 2017-06-12 12:57:12,834  Native.java:106 - Could not load JNR C Library, native system calls through this library will not be available (set this logger level to DEBUG to see the full stack trace).
INFO  [main] 2017-06-12 12:57:12,835  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:57:12,943  YamlConfigurationLoader.java:89 - Configuration location: file:/Users/russellspitzer/repos/bdp/resources/cassandra/conf/cassandra.yaml
DEBUG [main] 2017-06-12 12:57:12,945  YamlConfigurationLoader.java:108 - Loading settings from file:/Users/russellspitzer/repos/bdp/resources/cassandra/conf/cassandra.yaml
INFO  [main] 2017-06-12 12:57:13,221  Config.java:487 - Node configuration:[allocate_tokens_for_keyspace=null; allocate_tokens_for_local_replication_factor=null; authenticator=com.datastax.bdp.cassandra.auth.DseAuthenticator; authorizer=com.datastax.bdp.cassandra.auth.DseAuthorizer; auto_bootstrap=true; auto_snapshot=true; back_pressure_enabled=false; back_pressure_strategy=org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}; batch_size_fail_threshold_in_kb=640; batch_size_warn_threshold_in_kb=64; batchlog_replay_throttle_in_kb=1024; broadcast_address=null; broadcast_rpc_address=null; buffer_pool_use_heap_if_exhausted=true; cas_contention_timeout_in_ms=1000; cdc_enabled=false; cdc_free_space_check_interval_ms=250; cdc_raw_directory=/var/lib/cassandra/cdc_raw; cdc_total_space_in_mb=0; client_encryption_options=<REDACTED>; cluster_name=Test Cluster; column_index_cache_size_in_kb=2; column_index_size_in_kb=64; commit_failure_policy=stop; commitlog_compression=null; commitlog_directory=/var/lib/cassandra/commitlog; commitlog_max_compression_buffers_in_pool=3; commitlog_periodic_queue_size=-1; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_batch_window_in_ms=NaN; commitlog_sync_period_in_ms=10000; commitlog_total_space_in_mb=null; compaction_large_partition_warning_threshold_mb=100; compaction_throughput_mb_per_sec=16; concurrent_compactors=null; concurrent_counter_writes=32; concurrent_materialized_view_writes=32; concurrent_reads=32; concurrent_replicates=null; concurrent_writes=32; continuous_paging=org.apache.cassandra.config.ContinuousPagingConfig@9caca531; counter_cache_keys_to_save=2147483647; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; credentials_cache_max_entries=1000; credentials_update_interval_in_ms=-1; credentials_validity_in_ms=2000; cross_node_timeout=false; data_file_directories=[Ljava.lang.String;@acb0951; disk_access_mode=auto; disk_failure_policy=stop; disk_optimization_estimate_percentile=0.95; disk_optimization_page_cross_chance=0.1; disk_optimization_strategy=ssd; dynamic_snitch=true; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; enable_scripted_user_defined_functions=false; enable_user_defined_functions=false; enable_user_defined_functions_threads=true; encryption_options=null; endpoint_snitch=com.datastax.bdp.snitch.DseSimpleSnitch; file_cache_size_in_mb=null; gc_log_threshold_in_ms=200; gc_warn_threshold_in_ms=1000; hinted_handoff_disabled_datacenters=[]; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; hints_compression=null; hints_directory=/var/lib/cassandra/hints; hints_flush_period_in_ms=10000; incremental_backups=false; index_interval=null; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=null; inter_dc_stream_throughput_outbound_megabits_per_sec=200; inter_dc_tcp_nodelay=false; internode_authenticator=null; internode_compression=dc; internode_recv_buff_size_in_bytes=0; internode_send_buff_size_in_bytes=0; key_cache_keys_to_save=2147483647; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=localhost; listen_interface=null; listen_interface_prefer_ipv6=false; listen_on_broadcast_address=false; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; max_hints_file_size_in_mb=128; max_mutation_size_in_kb=null; max_streaming_retries=3; max_value_size_in_mb=256; memtable_allocation_type=heap_buffers; memtable_cleanup_threshold=null; memtable_flush_writers=0; memtable_heap_space_in_mb=null; memtable_offheap_space_in_mb=null; min_free_space_per_drive_in_mb=50; native_transport_max_concurrent_connections=-1; native_transport_max_concurrent_connections_per_ip=-1; native_transport_max_frame_size_in_mb=256; native_transport_max_threads=128; native_transport_port=9042; native_transport_port_ssl=null; num_tokens=1; otc_backlog_expiration_interval_ms=200; otc_coalescing_enough_coalesced_messages=8; otc_coalescing_strategy=DISABLED; otc_coalescing_window_us=200; partitioner=org.apache.cassandra.dht.Murmur3Partitioner; permissions_cache_max_entries=1000; permissions_update_interval_in_ms=-1; permissions_validity_in_ms=2000; phi_convict_threshold=8.0; prepared_statements_cache_size_mb=null; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=5000; request_scheduler=org.apache.cassandra.scheduler.NoScheduler; request_scheduler_id=null; request_scheduler_options=null; request_timeout_in_ms=10000; role_manager=com.datastax.bdp.cassandra.auth.DseRoleManager; roles_cache_max_entries=1000; roles_update_interval_in_ms=-1; roles_validity_in_ms=2000; row_cache_class_name=org.apache.cassandra.cache.OHCProvider; row_cache_keys_to_save=2147483647; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=localhost; rpc_interface=null; rpc_interface_prefer_ipv6=false; rpc_keepalive=true; rpc_listen_backlog=50; rpc_max_threads=2147483647; rpc_min_threads=16; rpc_port=9160; rpc_recv_buff_size_in_bytes=null; rpc_send_buff_size_in_bytes=null; rpc_server_type=sync; saved_caches_directory=/var/lib/cassandra/saved_caches; seed_provider=org.apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1}; server_encryption_options=<REDACTED>; slow_query_log_timeout_in_ms=500; snapshot_before_compaction=false; ssl_storage_port=7001; sstable_preemptive_open_interval_in_mb=50; start_native_transport=true; start_rpc=true; storage_port=7000; stream_throughput_outbound_megabits_per_sec=200; streaming_keep_alive_period_in_secs=300; streaming_socket_timeout_in_ms=86400000; thrift_framed_transport_size_in_mb=15; thrift_max_message_length_in_mb=16; thrift_prepared_statements_cache_size_mb=null; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; tracetype_query_ttl=86400; tracetype_repair_ttl=604800; transparent_data_encryption_options=org.apache.cassandra.config.TransparentDataEncryptionOptions@5bf22f18; trickle_fsync=true; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=60000; unlogged_batch_across_partitions_warn_threshold=10; user_defined_function_fail_timeout=1500; user_defined_function_warn_timeout=500; user_function_timeout_policy=die; windows_timer_interval=1; write_request_timeout_in_ms=2000]
DEBUG [main] 2017-06-12 12:57:13,222  DatabaseDescriptor.java:358 - Syncing log with a period of 10000
INFO  [main] 2017-06-12 12:57:13,222  DatabaseDescriptor.java:366 - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  [main] 2017-06-12 12:57:13,223  DatabaseDescriptor.java:420 - Global memtable on-heap threshold is enabled at 455MB
INFO  [main] 2017-06-12 12:57:13,223  DatabaseDescriptor.java:424 - Global memtable off-heap threshold is enabled at 455MB
INFO  [main] 2017-06-12 12:57:13,344  RateBasedBackPressure.java:123 - Initialized back-pressure with high ratio: 0.9, factor: 5, flow: FAST, window size: 2000.
INFO  [main] 2017-06-12 12:57:13,345  DatabaseDescriptor.java:717 - Back-pressure is disabled with strategy org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}.
INFO  [main] 2017-06-12 12:57:13,393  DseConfigYamlLoader.java:38 - Loading settings from file:/Users/russellspitzer/repos/bdp/resources/dse/conf/dse.yaml
WARN  [main] 2017-06-12 12:57:13,394  DseConfigYamlLoader.java:43 - Incorrect configuration loader detected (default), DSE functionality may be impaired
INFO  [main] 2017-06-12 12:57:13,499  DseConfig.java:382 - Load of settings is done.
INFO  [main] 2017-06-12 12:57:13,500  DseConfig.java:402 - CQL slow log is enabled
INFO  [main] 2017-06-12 12:57:13,500  DseConfig.java:403 - CQL system info tables are not enabled
INFO  [main] 2017-06-12 12:57:13,501  DseConfig.java:404 - Resource level latency tracking is not enabled
INFO  [main] 2017-06-12 12:57:13,501  DseConfig.java:405 - Database summary stats are not enabled
INFO  [main] 2017-06-12 12:57:13,501  DseConfig.java:406 - Cluster summary stats are not enabled
INFO  [main] 2017-06-12 12:57:13,502  DseConfig.java:407 - Histogram data tables are not enabled
INFO  [main] 2017-06-12 12:57:13,502  DseConfig.java:408 - User level latency tracking is not enabled
INFO  [main] 2017-06-12 12:57:13,503  DseConfig.java:410 - Spark cluster info tables are not enabled
INFO  [main] 2017-06-12 12:57:13,503  DseConfig.java:444 - Cql solr query paging is: off
INFO  [main] 2017-06-12 12:57:13,506  DseUtil.java:503 - /proc/cpuinfo is not available, defaulting to 1 thread per CPU core...
INFO  [main] 2017-06-12 12:57:13,506  DseConfig.java:448 - This instance appears to have 1 thread per CPU core and 8 total CPU threads.
INFO  [main] 2017-06-12 12:57:13,508  DseConfig.java:465 - Server ID:34-36-3B-CC-92-D8
INFO  [main] 2017-06-12 12:57:13,511  Clock.java:46 - Using java.lang.System clock to generate timestamps.
WARN  [main] 2017-06-12 12:57:13,640  NettyUtil.java:64 - Found Netty's native epoll transport, but not running on linux-based operating system. Using NIO instead.
INFO  [main] 2017-06-12 12:57:14,058  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:57:14,059  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 12:57:14,191  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:57:14,257  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:57:14,257  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 12:57:14,276  Logging.scala:35 - Trying to setup a server socket at /10.150.0.180:36583 to verify connectivity with DSE node...
INFO  [main] 2017-06-12 12:57:14,298  Logging.scala:35 - Successfully verified DSE Node -> this application connectivity on random port (36583)
INFO  [main] 2017-06-12 12:57:14,331  Logging.scala:54 - Running Spark version 2.0.2.6
WARN  [main] 2017-06-12 12:57:14,437  NativeCodeLoader.java:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
INFO  [main] 2017-06-12 12:57:14,517  Logging.scala:54 - Changing view acls to: russellspitzer
INFO  [main] 2017-06-12 12:57:14,518  Logging.scala:54 - Changing modify acls to: russellspitzer
INFO  [main] 2017-06-12 12:57:14,519  Logging.scala:54 - Changing view acls groups to: 
INFO  [main] 2017-06-12 12:57:14,519  Logging.scala:54 - Changing modify acls groups to: 
INFO  [main] 2017-06-12 12:57:14,520  Logging.scala:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(russellspitzer); groups with view permissions: Set(); users  with modify permissions: Set(russellspitzer); groups with modify permissions: Set()
INFO  [main] 2017-06-12 12:57:14,580  Logging.scala:54 - Successfully started service 'sparkDriver' on port 51892.
INFO  [main] 2017-06-12 12:57:14,597  Logging.scala:54 - Registering MapOutputTracker
INFO  [main] 2017-06-12 12:57:14,611  Logging.scala:54 - Registering BlockManagerMaster
INFO  [main] 2017-06-12 12:57:14,624  Logging.scala:54 - Created local directory at /private/var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/blockmgr-1210d68d-529f-4708-b52d-0c0cd64b8af8
INFO  [main] 2017-06-12 12:57:14,640  Logging.scala:54 - MemoryStore started with capacity 912.3 MB
INFO  [main] 2017-06-12 12:57:14,681  Logging.scala:54 - Registering OutputCommitCoordinator
INFO  [main] 2017-06-12 12:57:14,750  Log.java:186 - Logging initialized @3585ms
INFO  [main] 2017-06-12 12:57:14,825  Server.java:327 - jetty-9.2.z-SNAPSHOT
INFO  [main] 2017-06-12 12:57:14,842  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5f0f9947{/jobs,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,843  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@1aad0b1{/jobs/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,843  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@c3edf4c{/jobs/job,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,843  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5af8bb51{/jobs/job/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,844  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@799ed4e8{/stages,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,844  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@2e66bc32{/stages/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,844  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@54d8c20d{/stages/stage,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,845  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4b65d9f4{/stages/stage/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,845  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@44536de4{/stages/pool,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,845  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5fcfde70{/stages/pool/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,846  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4d95a72e{/storage,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,846  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@28da7d11{/storage/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,846  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@77b919a3{/storage/rdd,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,847  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5624657a{/storage/rdd/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,847  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@36681447{/environment,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,848  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4d21c56e{/environment/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,848  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@726aa968{/executors,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,848  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@7100dea{/executors/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,849  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@712cfb63{/executors/threadDump,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,849  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@32e54a9d{/executors/threadDump/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,856  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@15639440{/static,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,856  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@121bb45b{/,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,857  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4faa298{/api,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,857  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@1cd3b138{/stages/stage/kill,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:14,861  AbstractConnector.java:266 - Started ServerConnector@70972170{HTTP/1.1}{0.0.0.0:4040}
INFO  [main] 2017-06-12 12:57:14,861  Server.java:379 - Started @3697ms
INFO  [main] 2017-06-12 12:57:14,862  Logging.scala:54 - Successfully started service 'SparkUI' on port 4040.
INFO  [main] 2017-06-12 12:57:14,864  Logging.scala:54 - Bound SparkUI to 0.0.0.0, and started at http://10.150.0.180:4040
INFO  [main] 2017-06-12 12:57:14,919  DseClusterManager.scala:28 - Creating task scheduler: 10.150.0.180:51892
INFO  [main] 2017-06-12 12:57:14,927  DseClusterManager.scala:33 - Creating scheduler backend
INFO  [main] 2017-06-12 12:57:14,933  DseClusterManager.scala:38 - Initialization
INFO  [main] 2017-06-12 12:57:14,959  Logging.scala:54 - Using connector configuration: CassandraConnectorConf(Set(/127.0.0.1),9042,NoAuthConf,None,11000,1000,4000,None,,10,5000,120000,com.datastax.spark.connector.cql.DefaultConnectionFactory$@97e93f1,CassandraSSLConf(false,None,None,JKS,TLS,Set(TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA),false,None,None,JKS))
INFO  [dse-app-client-thread-pool-1] 2017-06-12 12:57:14,962  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [dse-app-client-thread-pool-1] 2017-06-12 12:57:15,013  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [dse-app-client-thread-pool-1] 2017-06-12 12:57:15,014  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [dse-app-client-thread-pool-2] 2017-06-12 12:57:15,079  Logging.scala:54 - Connected to Spark cluster with app ID app-20170612125715-0000
INFO  [main] 2017-06-12 12:57:15,086  Logging.scala:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51895.
INFO  [main] 2017-06-12 12:57:15,088  Logging.scala:54 - Server created on 10.150.0.180:51895
INFO  [main] 2017-06-12 12:57:15,091  Logging.scala:54 - external shuffle service port = 7437
INFO  [main] 2017-06-12 12:57:15,093  Logging.scala:54 - Registering BlockManager BlockManagerId(driver, 10.150.0.180, 51895)
INFO  [dispatcher-event-loop-3] 2017-06-12 12:57:15,096  Logging.scala:54 - Registering block manager 10.150.0.180:51895 with 912.3 MB RAM, BlockManagerId(driver, 10.150.0.180, 51895)
INFO  [main] 2017-06-12 12:57:15,099  Logging.scala:54 - Registered BlockManager BlockManagerId(driver, 10.150.0.180, 51895)
INFO  [main] 2017-06-12 12:57:15,255  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@7af3874e{/metrics/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:15,267  Logging.scala:54 - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
WARN  [main] 2017-06-12 12:57:15,270  Logging.scala:66 - Use an existing SparkContext, some configuration may not take effect.
INFO  [main] 2017-06-12 12:57:15,284  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@2b5c4f17{/SQL,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:15,284  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@69f0b0f4{/SQL/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:15,285  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@177c41d7{/SQL/execution,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:15,286  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@1d61a348{/SQL/execution/json,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:15,287  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@8cc8cdb{/static/sql,null,AVAILABLE}
INFO  [main] 2017-06-12 12:57:15,323  Logging.scala:54 - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('dsefs:///user/spark/warehouse').
INFO  [main] 2017-06-12 12:57:15,325  Logging.scala:54 - Warehouse path is 'dsefs:/user/spark/warehouse'.
INFO  [main] 2017-06-12 12:57:16,224  Logging.scala:54 - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
INFO  [main] 2017-06-12 12:57:16,931  HiveMetaStore.java:589 - 0: Opening raw store with implemenation class:com.datastax.bdp.hadoop.hive.metastore.CassandraHiveMetaStore
INFO  [main] 2017-06-12 12:57:17,015  GuavaCompatibility.java:126 - Detected Guava < 19 in the classpath, using legacy compatibility layer
INFO  [main] 2017-06-12 12:57:17,054  DseConnectionUtil.java:175 - Using no credentials
INFO  [main] 2017-06-12 12:57:17,063  Native.java:106 - Could not load JNR C Library, native system calls through this library will not be available (set this logger level to DEBUG to see the full stack trace).
INFO  [main] 2017-06-12 12:57:17,064  Clock.java:46 - Using java.lang.System clock to generate timestamps.
WARN  [main] 2017-06-12 12:57:17,217  NettyUtil.java:64 - Found Netty's native epoll transport, but not running on linux-based operating system. Using NIO instead.
INFO  [main] 2017-06-12 12:57:17,579  DCAwareRoundRobinPolicy.java:86 - Using data-center name 'Analytics' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)
INFO  [main] 2017-06-12 12:57:17,580  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:57:17,630  HiveMetaStore.java:663 - Added admin role in metastore
INFO  [main] 2017-06-12 12:57:17,633  HiveMetaStore.java:672 - Added public role in metastore
INFO  [main] 2017-06-12 12:57:17,642  HiveMetaStore.java:712 - No user is added in admin role, since config is empty
INFO  [main] 2017-06-12 12:57:17,722  HiveMetaStore.java:746 - 0: get_all_databases
INFO  [main] 2017-06-12 12:57:17,724  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=get_all_databases	
INFO  [main] 2017-06-12 12:57:17,726  SchemaManagerService.java:219 - Updating Cassandra Keyspace to Metastore Database Mapping
INFO  [main] 2017-06-12 12:57:17,726  SchemaManagerService.java:89 - Refresh cluster meta data
INFO  [main] 2017-06-12 12:57:17,728  DseConnectionUtil.java:175 - Using no credentials
INFO  [main] 2017-06-12 12:57:17,728  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 12:57:17,813  DCAwareRoundRobinPolicy.java:86 - Using data-center name 'Analytics' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)
INFO  [main] 2017-06-12 12:57:17,815  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 12:57:17,855  HiveMetaStore.java:746 - 0: get_functions: db=default pat=*
INFO  [main] 2017-06-12 12:57:17,856  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
INFO  [main] 2017-06-12 12:57:17,856  CassandraHiveMetaStore.java:1515 - in getFunctions with dbName: default and functionNamePattern: *
INFO  [main] 2017-06-12 12:57:17,864  HiveMetaStore.java:746 - 0: get_functions: db=dsefs pat=*
INFO  [main] 2017-06-12 12:57:17,869  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=get_functions: db=dsefs pat=*	
INFO  [main] 2017-06-12 12:57:17,870  CassandraHiveMetaStore.java:1515 - in getFunctions with dbName: dsefs and functionNamePattern: *
INFO  [main] 2017-06-12 12:57:17,873  HiveMetaStore.java:746 - 0: get_functions: db=ks pat=*
INFO  [main] 2017-06-12 12:57:17,874  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=get_functions: db=ks pat=*	
INFO  [main] 2017-06-12 12:57:17,875  CassandraHiveMetaStore.java:1515 - in getFunctions with dbName: ks and functionNamePattern: *
INFO  [main] 2017-06-12 12:57:18,039  DseRestClientAuthProviderBuilderFactory.scala:35 - Using no authentication for DSEFS
INFO  [main] 2017-06-12 12:57:18,490  SessionState.java:641 - Created local directory: /var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/8413905c-5a3e-4c22-b56b-48ce1a26e527_resources
INFO  [main] 2017-06-12 12:57:18,680  SessionState.java:641 - Created HDFS directory: /tmp/hive/russellspitzer/8413905c-5a3e-4c22-b56b-48ce1a26e527
INFO  [main] 2017-06-12 12:57:18,723  SessionState.java:641 - Created local directory: /var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/russellspitzer/8413905c-5a3e-4c22-b56b-48ce1a26e527
INFO  [main] 2017-06-12 12:57:18,793  SessionState.java:641 - Created HDFS directory: /tmp/hive/russellspitzer/8413905c-5a3e-4c22-b56b-48ce1a26e527/_tmp_space.db
INFO  [main] 2017-06-12 12:57:18,823  Logging.scala:54 - Warehouse location for Hive client (version 1.2.1) is dsefs:///user/spark/warehouse
INFO  [main] 2017-06-12 12:57:19,002  SessionState.java:641 - Created local directory: /var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/29ee58fd-fcd6-440f-9014-d0d363f20146_resources
INFO  [main] 2017-06-12 12:57:19,070  SessionState.java:641 - Created HDFS directory: /tmp/hive/russellspitzer/29ee58fd-fcd6-440f-9014-d0d363f20146
INFO  [main] 2017-06-12 12:57:19,104  SessionState.java:641 - Created local directory: /var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/russellspitzer/29ee58fd-fcd6-440f-9014-d0d363f20146
INFO  [main] 2017-06-12 12:57:19,168  SessionState.java:641 - Created HDFS directory: /tmp/hive/russellspitzer/29ee58fd-fcd6-440f-9014-d0d363f20146/_tmp_space.db
INFO  [main] 2017-06-12 12:57:19,193  Logging.scala:54 - Warehouse location for Hive client (version 1.2.1) is dsefs:///user/spark/warehouse
INFO  [main] 2017-06-12 12:57:19,380  HiveMetaStore.java:746 - 0: create_database: Database(name:default, description:default database, locationUri:dsefs://127.0.0.1/user/spark/warehouse, parameters:{})
INFO  [main] 2017-06-12 12:57:19,380  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:dsefs://127.0.0.1/user/spark/warehouse, parameters:{})	
ERROR [main] 2017-06-12 12:57:19,389  RetryingHMSHandler.java:159 - AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy17.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy18.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:280)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:269)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:308)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:51)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:161)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)
	at org.apache.spark.sql.SparkSession.range(SparkSession.scala:509)
	at org.apache.spark.sql.SparkSession.range(SparkSession.scala:484)
	at com.datastax.spark.example.WriteRead$.delayedEndpoint$com$datastax$spark$example$WriteRead$1(WriteRead.scala:31)
	at com.datastax.spark.example.WriteRead$delayedInit$body.apply(WriteRead.scala:11)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.App$$anonfun$main$1.apply(App.scala:76)
	at scala.App$$anonfun$main$1.apply(App.scala:76)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
	at scala.App$class.main(App.scala:76)
	at com.datastax.spark.example.WriteRead$.main(WriteRead.scala:11)
	at com.datastax.spark.example.WriteRead.main(WriteRead.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at scala.reflect.internal.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:70)
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
	at scala.reflect.internal.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:101)
	at scala.reflect.internal.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:70)
	at scala.reflect.internal.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101)
	at scala.tools.nsc.CommonRunner$class.run(ObjectRunner.scala:22)
	at scala.tools.nsc.JarRunner$.run(MainGenericRunner.scala:13)
	at scala.tools.nsc.CommonRunner$class.runAndCatch(ObjectRunner.scala:29)
	at scala.tools.nsc.JarRunner$.runJar(MainGenericRunner.scala:25)
	at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:69)
	at scala.tools.nsc.MainGenericRunner.run$1(MainGenericRunner.scala:87)
	at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:98)
	at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:103)
	at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala)

INFO  [main] 2017-06-12 12:57:20,007  Logging.scala:54 - Code generated in 48.521187 ms
INFO  [main] 2017-06-12 12:57:20,322  Logging.scala:54 - Starting job: runJob at RDDFunctions.scala:36
INFO  [dag-scheduler-event-loop] 2017-06-12 12:57:20,337  Logging.scala:54 - Got job 0 (runJob at RDDFunctions.scala:36) with 2 output partitions
INFO  [dag-scheduler-event-loop] 2017-06-12 12:57:20,338  Logging.scala:54 - Final stage: ResultStage 0 (runJob at RDDFunctions.scala:36)
INFO  [dag-scheduler-event-loop] 2017-06-12 12:57:20,338  Logging.scala:54 - Parents of final stage: List()
INFO  [dag-scheduler-event-loop] 2017-06-12 12:57:20,340  Logging.scala:54 - Missing parents: List()
INFO  [dag-scheduler-event-loop] 2017-06-12 12:57:20,347  Logging.scala:54 - Submitting ResultStage 0 (MapPartitionsRDD[5] at rdd at WriteRead.scala:33), which has no missing parents
INFO  [dag-scheduler-event-loop] 2017-06-12 12:57:20,455  Logging.scala:54 - Block broadcast_0 stored as values in memory (estimated size 18.0 KB, free 912.3 MB)
INFO  [dag-scheduler-event-loop] 2017-06-12 12:57:20,481  Logging.scala:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 8.4 KB, free 912.3 MB)
INFO  [dispatcher-event-loop-3] 2017-06-12 12:57:20,482  Logging.scala:54 - Added broadcast_0_piece0 in memory on 10.150.0.180:51895 (size: 8.4 KB, free: 912.3 MB)
INFO  [dag-scheduler-event-loop] 2017-06-12 12:57:20,488  Logging.scala:54 - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
INFO  [dag-scheduler-event-loop] 2017-06-12 12:57:20,494  Logging.scala:54 - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at rdd at WriteRead.scala:33)
INFO  [dag-scheduler-event-loop] 2017-06-12 12:57:20,495  Logging.scala:54 - Adding task set 0.0 with 2 tasks
INFO  [pool-1-thread-1] 2017-06-12 12:57:27,470  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [pool-1-thread-1] 2017-06-12 12:57:29,684  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
WARN  [Timer-0] 2017-06-12 12:57:35,504  Logging.scala:66 - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
INFO  [dispatcher-event-loop-3] 2017-06-12 12:57:49,371  Logging.scala:54 - Registered executor NettyRpcEndpointRef(null) (10.150.0.180:51923) with ID 0
INFO  [dispatcher-event-loop-3] 2017-06-12 12:57:49,405  Logging.scala:54 - Starting task 0.0 in stage 0.0 (TID 0, 127.0.0.1, partition 0, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-3] 2017-06-12 12:57:49,408  Logging.scala:54 - Starting task 1.0 in stage 0.0 (TID 1, 127.0.0.1, partition 1, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-3] 2017-06-12 12:57:49,419  Logging.scala:54 - Launching task 0 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-3] 2017-06-12 12:57:49,420  Logging.scala:54 - Launching task 1 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-4] 2017-06-12 12:57:49,446  Logging.scala:54 - Registering block manager 127.0.0.1:51924 with 366.3 MB RAM, BlockManagerId(0, 127.0.0.1, 51924)
INFO  [dispatcher-event-loop-1] 2017-06-12 12:57:49,691  Logging.scala:54 - Added broadcast_0_piece0 in memory on 127.0.0.1:51924 (size: 8.4 KB, free: 366.3 MB)
WARN  [task-result-getter-0] 2017-06-12 12:57:49,952  Logging.scala:66 - Lost task 1.0 in stage 0.0 (TID 1, 127.0.0.1): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2089)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1261)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2006)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

INFO  [task-result-getter-1] 2017-06-12 12:57:49,955  Logging.scala:54 - Lost task 0.0 in stage 0.0 (TID 0) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 1]
INFO  [dispatcher-event-loop-2] 2017-06-12 12:57:49,957  Logging.scala:54 - Starting task 0.1 in stage 0.0 (TID 2, 127.0.0.1, partition 0, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-2] 2017-06-12 12:57:49,958  Logging.scala:54 - Starting task 1.1 in stage 0.0 (TID 3, 127.0.0.1, partition 1, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-2] 2017-06-12 12:57:49,959  Logging.scala:54 - Launching task 2 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-2] 2017-06-12 12:57:49,959  Logging.scala:54 - Launching task 3 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-2] 2017-06-12 12:57:49,975  Logging.scala:54 - Lost task 0.1 in stage 0.0 (TID 2) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 2]
INFO  [dispatcher-event-loop-6] 2017-06-12 12:57:49,977  Logging.scala:54 - Starting task 0.2 in stage 0.0 (TID 4, 127.0.0.1, partition 0, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-6] 2017-06-12 12:57:49,977  Logging.scala:54 - Launching task 4 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-3] 2017-06-12 12:57:49,977  Logging.scala:54 - Lost task 1.1 in stage 0.0 (TID 3) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 3]
INFO  [dispatcher-event-loop-4] 2017-06-12 12:57:49,979  Logging.scala:54 - Starting task 1.2 in stage 0.0 (TID 5, 127.0.0.1, partition 1, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-4] 2017-06-12 12:57:49,979  Logging.scala:54 - Launching task 5 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-0] 2017-06-12 12:57:50,001  Logging.scala:54 - Lost task 0.2 in stage 0.0 (TID 4) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 4]
INFO  [dispatcher-event-loop-0] 2017-06-12 12:57:50,002  Logging.scala:54 - Starting task 0.3 in stage 0.0 (TID 6, 127.0.0.1, partition 0, PROCESS_LOCAL, 5289 bytes)
INFO  [dispatcher-event-loop-0] 2017-06-12 12:57:50,002  Logging.scala:54 - Launching task 6 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-1] 2017-06-12 12:57:50,013  Logging.scala:54 - Lost task 1.2 in stage 0.0 (TID 5) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 5]
INFO  [dispatcher-event-loop-6] 2017-06-12 12:57:50,014  Logging.scala:54 - Starting task 1.3 in stage 0.0 (TID 7, 127.0.0.1, partition 1, PROCESS_LOCAL, 5289 bytes)
INFO  [task-result-getter-2] 2017-06-12 12:57:50,015  Logging.scala:54 - Lost task 0.3 in stage 0.0 (TID 6) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 6]
INFO  [dispatcher-event-loop-6] 2017-06-12 12:57:50,015  Logging.scala:54 - Launching task 7 on executor id: 0 hostname: 127.0.0.1.
ERROR [task-result-getter-2] 2017-06-12 12:57:50,016  Logging.scala:70 - Task 0 in stage 0.0 failed 4 times; aborting job
INFO  [dag-scheduler-event-loop] 2017-06-12 12:57:50,020  Logging.scala:54 - Cancelling stage 0
INFO  [dag-scheduler-event-loop] 2017-06-12 12:57:50,025  Logging.scala:54 - Stage 0 was cancelled
INFO  [dag-scheduler-event-loop] 2017-06-12 12:57:50,026  Logging.scala:54 - ResultStage 0 (runJob at RDDFunctions.scala:36) failed in 29.522 s
INFO  [main] 2017-06-12 12:57:50,028  Logging.scala:54 - Job 0 failed: runJob at RDDFunctions.scala:36, took 29.706225 s
INFO  [task-result-getter-3] 2017-06-12 12:57:50,033  Logging.scala:54 - Lost task 1.3 in stage 0.0 (TID 7) on executor 127.0.0.1: java.lang.ClassCastException (cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 7]
INFO  [task-result-getter-3] 2017-06-12 12:57:50,038  Logging.scala:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
INFO  [Thread-2] 2017-06-12 12:57:50,039  Logging.scala:54 - Invoking stop() from shutdown hook
INFO  [Thread-2] 2017-06-12 12:57:50,049  AbstractConnector.java:306 - Stopped ServerConnector@70972170{HTTP/1.1}{0.0.0.0:4040}
INFO  [Thread-2] 2017-06-12 12:57:50,050  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@1cd3b138{/stages/stage/kill,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,050  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4faa298{/api,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,051  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@121bb45b{/,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,051  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@15639440{/static,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,051  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@32e54a9d{/executors/threadDump/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,051  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@712cfb63{/executors/threadDump,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,052  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@7100dea{/executors/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,052  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@726aa968{/executors,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,052  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4d21c56e{/environment/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,052  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@36681447{/environment,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,052  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5624657a{/storage/rdd/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,053  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@77b919a3{/storage/rdd,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,053  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@28da7d11{/storage/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,053  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4d95a72e{/storage,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,053  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5fcfde70{/stages/pool/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,054  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@44536de4{/stages/pool,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,054  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4b65d9f4{/stages/stage/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,054  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@54d8c20d{/stages/stage,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,054  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@2e66bc32{/stages/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,055  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@799ed4e8{/stages,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,055  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5af8bb51{/jobs/job/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,055  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@c3edf4c{/jobs/job,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,055  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@1aad0b1{/jobs/json,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,056  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5f0f9947{/jobs,null,UNAVAILABLE}
INFO  [Thread-2] 2017-06-12 12:57:50,057  Logging.scala:54 - Stopped Spark web UI at http://10.150.0.180:4040
INFO  [dse-app-client-thread-pool-2] 2017-06-12 12:57:50,065  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [dse-app-client-thread-pool-2] 2017-06-12 12:57:50,106  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [dse-app-client-thread-pool-2] 2017-06-12 12:57:50,106  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [Serial shutdown hooks thread] 2017-06-12 12:57:52,259  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [Serial shutdown hooks thread] 2017-06-12 12:57:52,260  Logging.scala:35 - Successfully executed shutdown hook: Clearing session cache for C* connector
INFO  [dse-app-client-thread-pool-2] 2017-06-12 12:57:52,355  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [Thread-2] 2017-06-12 12:57:52,356  Logging.scala:54 - Shutting down all executors
INFO  [dispatcher-event-loop-4] 2017-06-12 12:57:52,357  Logging.scala:54 - Asking each executor to shut down
INFO  [dse-app-client-thread-pool-0] 2017-06-12 12:57:55,082  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [dse-app-client-thread-pool-0] 2017-06-12 12:57:55,131  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [dse-app-client-thread-pool-0] 2017-06-12 12:57:55,131  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [dse-app-client-thread-pool-0] 2017-06-12 12:57:57,367  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [dispatcher-event-loop-6] 2017-06-12 12:57:57,428  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [dispatcher-event-loop-6] 2017-06-12 12:57:57,459  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [dispatcher-event-loop-6] 2017-06-12 12:57:57,459  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [dispatcher-event-loop-6] 2017-06-12 12:57:59,682  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [dispatcher-event-loop-6] 2017-06-12 12:57:59,684  Logging.scala:54 - Shutting down the thread pool
INFO  [dispatcher-event-loop-6] 2017-06-12 12:57:59,684  Logging.scala:54 - Evicting connector cache
INFO  [dispatcher-event-loop-6] 2017-06-12 12:57:59,686  Logging.scala:54 - onStop complete
INFO  [dispatcher-event-loop-0] 2017-06-12 12:57:59,688  Logging.scala:54 - MapOutputTrackerMasterEndpoint stopped!
INFO  [Thread-2] 2017-06-12 12:57:59,693  Logging.scala:54 - MemoryStore cleared
INFO  [Thread-2] 2017-06-12 12:57:59,694  Logging.scala:54 - BlockManager stopped
INFO  [Thread-2] 2017-06-12 12:57:59,697  Logging.scala:54 - BlockManagerMaster stopped
INFO  [dispatcher-event-loop-2] 2017-06-12 12:57:59,699  Logging.scala:54 - OutputCommitCoordinator stopped!
INFO  [Thread-2] 2017-06-12 12:57:59,700  Logging.scala:54 - Successfully stopped SparkContext
INFO  [Thread-2] 2017-06-12 12:57:59,701  Logging.scala:54 - Shutdown hook called
INFO  [Thread-2] 2017-06-12 12:57:59,702  Logging.scala:54 - Deleting directory /private/var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/spark-7158dedc-8820-4866-ae61-2d79a90e19e3
INFO  [main] 2017-06-12 13:04:14,055  GuavaCompatibility.java:126 - Detected Guava < 19 in the classpath, using legacy compatibility layer
INFO  [main] 2017-06-12 13:04:14,079  Native.java:106 - Could not load JNR C Library, native system calls through this library will not be available (set this logger level to DEBUG to see the full stack trace).
INFO  [main] 2017-06-12 13:04:14,079  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 13:04:14,189  YamlConfigurationLoader.java:89 - Configuration location: file:/Users/russellspitzer/repos/bdp/resources/cassandra/conf/cassandra.yaml
DEBUG [main] 2017-06-12 13:04:14,191  YamlConfigurationLoader.java:108 - Loading settings from file:/Users/russellspitzer/repos/bdp/resources/cassandra/conf/cassandra.yaml
INFO  [main] 2017-06-12 13:04:14,469  Config.java:487 - Node configuration:[allocate_tokens_for_keyspace=null; allocate_tokens_for_local_replication_factor=null; authenticator=com.datastax.bdp.cassandra.auth.DseAuthenticator; authorizer=com.datastax.bdp.cassandra.auth.DseAuthorizer; auto_bootstrap=true; auto_snapshot=true; back_pressure_enabled=false; back_pressure_strategy=org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}; batch_size_fail_threshold_in_kb=640; batch_size_warn_threshold_in_kb=64; batchlog_replay_throttle_in_kb=1024; broadcast_address=null; broadcast_rpc_address=null; buffer_pool_use_heap_if_exhausted=true; cas_contention_timeout_in_ms=1000; cdc_enabled=false; cdc_free_space_check_interval_ms=250; cdc_raw_directory=/var/lib/cassandra/cdc_raw; cdc_total_space_in_mb=0; client_encryption_options=<REDACTED>; cluster_name=Test Cluster; column_index_cache_size_in_kb=2; column_index_size_in_kb=64; commit_failure_policy=stop; commitlog_compression=null; commitlog_directory=/var/lib/cassandra/commitlog; commitlog_max_compression_buffers_in_pool=3; commitlog_periodic_queue_size=-1; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_batch_window_in_ms=NaN; commitlog_sync_period_in_ms=10000; commitlog_total_space_in_mb=null; compaction_large_partition_warning_threshold_mb=100; compaction_throughput_mb_per_sec=16; concurrent_compactors=null; concurrent_counter_writes=32; concurrent_materialized_view_writes=32; concurrent_reads=32; concurrent_replicates=null; concurrent_writes=32; continuous_paging=org.apache.cassandra.config.ContinuousPagingConfig@9caca531; counter_cache_keys_to_save=2147483647; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; credentials_cache_max_entries=1000; credentials_update_interval_in_ms=-1; credentials_validity_in_ms=2000; cross_node_timeout=false; data_file_directories=[Ljava.lang.String;@acb0951; disk_access_mode=auto; disk_failure_policy=stop; disk_optimization_estimate_percentile=0.95; disk_optimization_page_cross_chance=0.1; disk_optimization_strategy=ssd; dynamic_snitch=true; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; enable_scripted_user_defined_functions=false; enable_user_defined_functions=false; enable_user_defined_functions_threads=true; encryption_options=null; endpoint_snitch=com.datastax.bdp.snitch.DseSimpleSnitch; file_cache_size_in_mb=null; gc_log_threshold_in_ms=200; gc_warn_threshold_in_ms=1000; hinted_handoff_disabled_datacenters=[]; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; hints_compression=null; hints_directory=/var/lib/cassandra/hints; hints_flush_period_in_ms=10000; incremental_backups=false; index_interval=null; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=null; inter_dc_stream_throughput_outbound_megabits_per_sec=200; inter_dc_tcp_nodelay=false; internode_authenticator=null; internode_compression=dc; internode_recv_buff_size_in_bytes=0; internode_send_buff_size_in_bytes=0; key_cache_keys_to_save=2147483647; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=localhost; listen_interface=null; listen_interface_prefer_ipv6=false; listen_on_broadcast_address=false; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; max_hints_file_size_in_mb=128; max_mutation_size_in_kb=null; max_streaming_retries=3; max_value_size_in_mb=256; memtable_allocation_type=heap_buffers; memtable_cleanup_threshold=null; memtable_flush_writers=0; memtable_heap_space_in_mb=null; memtable_offheap_space_in_mb=null; min_free_space_per_drive_in_mb=50; native_transport_max_concurrent_connections=-1; native_transport_max_concurrent_connections_per_ip=-1; native_transport_max_frame_size_in_mb=256; native_transport_max_threads=128; native_transport_port=9042; native_transport_port_ssl=null; num_tokens=1; otc_backlog_expiration_interval_ms=200; otc_coalescing_enough_coalesced_messages=8; otc_coalescing_strategy=DISABLED; otc_coalescing_window_us=200; partitioner=org.apache.cassandra.dht.Murmur3Partitioner; permissions_cache_max_entries=1000; permissions_update_interval_in_ms=-1; permissions_validity_in_ms=2000; phi_convict_threshold=8.0; prepared_statements_cache_size_mb=null; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=5000; request_scheduler=org.apache.cassandra.scheduler.NoScheduler; request_scheduler_id=null; request_scheduler_options=null; request_timeout_in_ms=10000; role_manager=com.datastax.bdp.cassandra.auth.DseRoleManager; roles_cache_max_entries=1000; roles_update_interval_in_ms=-1; roles_validity_in_ms=2000; row_cache_class_name=org.apache.cassandra.cache.OHCProvider; row_cache_keys_to_save=2147483647; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=localhost; rpc_interface=null; rpc_interface_prefer_ipv6=false; rpc_keepalive=true; rpc_listen_backlog=50; rpc_max_threads=2147483647; rpc_min_threads=16; rpc_port=9160; rpc_recv_buff_size_in_bytes=null; rpc_send_buff_size_in_bytes=null; rpc_server_type=sync; saved_caches_directory=/var/lib/cassandra/saved_caches; seed_provider=org.apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1}; server_encryption_options=<REDACTED>; slow_query_log_timeout_in_ms=500; snapshot_before_compaction=false; ssl_storage_port=7001; sstable_preemptive_open_interval_in_mb=50; start_native_transport=true; start_rpc=true; storage_port=7000; stream_throughput_outbound_megabits_per_sec=200; streaming_keep_alive_period_in_secs=300; streaming_socket_timeout_in_ms=86400000; thrift_framed_transport_size_in_mb=15; thrift_max_message_length_in_mb=16; thrift_prepared_statements_cache_size_mb=null; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; tracetype_query_ttl=86400; tracetype_repair_ttl=604800; transparent_data_encryption_options=org.apache.cassandra.config.TransparentDataEncryptionOptions@5bf22f18; trickle_fsync=true; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=60000; unlogged_batch_across_partitions_warn_threshold=10; user_defined_function_fail_timeout=1500; user_defined_function_warn_timeout=500; user_function_timeout_policy=die; windows_timer_interval=1; write_request_timeout_in_ms=2000]
DEBUG [main] 2017-06-12 13:04:14,470  DatabaseDescriptor.java:358 - Syncing log with a period of 10000
INFO  [main] 2017-06-12 13:04:14,470  DatabaseDescriptor.java:366 - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  [main] 2017-06-12 13:04:14,471  DatabaseDescriptor.java:420 - Global memtable on-heap threshold is enabled at 455MB
INFO  [main] 2017-06-12 13:04:14,471  DatabaseDescriptor.java:424 - Global memtable off-heap threshold is enabled at 455MB
INFO  [main] 2017-06-12 13:04:14,602  RateBasedBackPressure.java:123 - Initialized back-pressure with high ratio: 0.9, factor: 5, flow: FAST, window size: 2000.
INFO  [main] 2017-06-12 13:04:14,602  DatabaseDescriptor.java:717 - Back-pressure is disabled with strategy org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}.
INFO  [main] 2017-06-12 13:04:14,656  DseConfigYamlLoader.java:38 - Loading settings from file:/Users/russellspitzer/repos/bdp/resources/dse/conf/dse.yaml
WARN  [main] 2017-06-12 13:04:14,657  DseConfigYamlLoader.java:43 - Incorrect configuration loader detected (default), DSE functionality may be impaired
INFO  [main] 2017-06-12 13:04:14,767  DseConfig.java:382 - Load of settings is done.
INFO  [main] 2017-06-12 13:04:14,768  DseConfig.java:402 - CQL slow log is enabled
INFO  [main] 2017-06-12 13:04:14,768  DseConfig.java:403 - CQL system info tables are not enabled
INFO  [main] 2017-06-12 13:04:14,769  DseConfig.java:404 - Resource level latency tracking is not enabled
INFO  [main] 2017-06-12 13:04:14,769  DseConfig.java:405 - Database summary stats are not enabled
INFO  [main] 2017-06-12 13:04:14,769  DseConfig.java:406 - Cluster summary stats are not enabled
INFO  [main] 2017-06-12 13:04:14,770  DseConfig.java:407 - Histogram data tables are not enabled
INFO  [main] 2017-06-12 13:04:14,770  DseConfig.java:408 - User level latency tracking is not enabled
INFO  [main] 2017-06-12 13:04:14,770  DseConfig.java:410 - Spark cluster info tables are not enabled
INFO  [main] 2017-06-12 13:04:14,771  DseConfig.java:444 - Cql solr query paging is: off
INFO  [main] 2017-06-12 13:04:14,773  DseUtil.java:503 - /proc/cpuinfo is not available, defaulting to 1 thread per CPU core...
INFO  [main] 2017-06-12 13:04:14,773  DseConfig.java:448 - This instance appears to have 1 thread per CPU core and 8 total CPU threads.
INFO  [main] 2017-06-12 13:04:14,775  DseConfig.java:465 - Server ID:34-36-3B-CC-92-D8
INFO  [main] 2017-06-12 13:04:14,777  Clock.java:46 - Using java.lang.System clock to generate timestamps.
WARN  [main] 2017-06-12 13:04:14,908  NettyUtil.java:64 - Found Netty's native epoll transport, but not running on linux-based operating system. Using NIO instead.
INFO  [main] 2017-06-12 13:04:15,313  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 13:04:15,314  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 13:04:15,432  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 13:04:15,486  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 13:04:15,487  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 13:04:15,504  Logging.scala:35 - Trying to setup a server socket at /10.150.0.180:39589 to verify connectivity with DSE node...
INFO  [main] 2017-06-12 13:04:15,519  Logging.scala:35 - Successfully verified DSE Node -> this application connectivity on random port (39589)
INFO  [main] 2017-06-12 13:04:15,547  Logging.scala:54 - Running Spark version 2.0.2.6
WARN  [main] 2017-06-12 13:04:15,652  NativeCodeLoader.java:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
INFO  [main] 2017-06-12 13:04:15,726  Logging.scala:54 - Changing view acls to: russellspitzer
INFO  [main] 2017-06-12 13:04:15,726  Logging.scala:54 - Changing modify acls to: russellspitzer
INFO  [main] 2017-06-12 13:04:15,727  Logging.scala:54 - Changing view acls groups to: 
INFO  [main] 2017-06-12 13:04:15,727  Logging.scala:54 - Changing modify acls groups to: 
INFO  [main] 2017-06-12 13:04:15,728  Logging.scala:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(russellspitzer); groups with view permissions: Set(); users  with modify permissions: Set(russellspitzer); groups with modify permissions: Set()
INFO  [main] 2017-06-12 13:04:15,785  Logging.scala:54 - Successfully started service 'sparkDriver' on port 52043.
INFO  [main] 2017-06-12 13:04:15,801  Logging.scala:54 - Registering MapOutputTracker
INFO  [main] 2017-06-12 13:04:15,815  Logging.scala:54 - Registering BlockManagerMaster
INFO  [main] 2017-06-12 13:04:15,827  Logging.scala:54 - Created local directory at /private/var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/blockmgr-d9b269e3-00c8-45c0-ac12-ede017f59fbf
INFO  [main] 2017-06-12 13:04:15,842  Logging.scala:54 - MemoryStore started with capacity 912.3 MB
INFO  [main] 2017-06-12 13:04:15,878  Logging.scala:54 - Registering OutputCommitCoordinator
INFO  [main] 2017-06-12 13:04:15,947  Log.java:186 - Logging initialized @3605ms
INFO  [main] 2017-06-12 13:04:16,018  Server.java:327 - jetty-9.2.z-SNAPSHOT
INFO  [main] 2017-06-12 13:04:16,032  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@c3edf4c{/jobs,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,032  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5af8bb51{/jobs/json,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,033  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@799ed4e8{/jobs/job,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,033  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@2e66bc32{/jobs/job/json,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,033  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@54d8c20d{/stages,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,033  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4b65d9f4{/stages/json,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,034  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@44536de4{/stages/stage,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,034  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5fcfde70{/stages/stage/json,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,034  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4d95a72e{/stages/pool,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,034  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@28da7d11{/stages/pool/json,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,035  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@77b919a3{/storage,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,035  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5624657a{/storage/json,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,036  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@36681447{/storage/rdd,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,036  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4d21c56e{/storage/rdd/json,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,037  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@726aa968{/environment,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,037  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@7100dea{/environment/json,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,038  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@712cfb63{/executors,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,038  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@32e54a9d{/executors/json,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,039  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@15639440{/executors/threadDump,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,039  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@121bb45b{/executors/threadDump/json,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,046  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4faa298{/static,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,047  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@1cd3b138{/,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,048  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@151bf776{/api,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,048  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@5a6d30e2{/stages/stage/kill,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,052  AbstractConnector.java:266 - Started ServerConnector@4e1a46fb{HTTP/1.1}{0.0.0.0:4040}
INFO  [main] 2017-06-12 13:04:16,053  Server.java:379 - Started @3711ms
INFO  [main] 2017-06-12 13:04:16,054  Logging.scala:54 - Successfully started service 'SparkUI' on port 4040.
INFO  [main] 2017-06-12 13:04:16,056  Logging.scala:54 - Bound SparkUI to 0.0.0.0, and started at http://10.150.0.180:4040
INFO  [main] 2017-06-12 13:04:16,096  Logging.scala:54 - Added JAR target/scala-2.11/writeread_2.11-0.1.jar at spark://10.150.0.180:52043/jars/writeread_2.11-0.1.jar with timestamp 1497297856095
INFO  [main] 2017-06-12 13:04:16,110  DseClusterManager.scala:28 - Creating task scheduler: 10.150.0.180:52043
INFO  [main] 2017-06-12 13:04:16,120  DseClusterManager.scala:33 - Creating scheduler backend
INFO  [main] 2017-06-12 13:04:16,127  DseClusterManager.scala:38 - Initialization
INFO  [main] 2017-06-12 13:04:16,152  Logging.scala:54 - Using connector configuration: CassandraConnectorConf(Set(/127.0.0.1),9042,NoAuthConf,None,11000,1000,4000,None,,10,5000,120000,com.datastax.spark.connector.cql.DefaultConnectionFactory$@97e93f1,CassandraSSLConf(false,None,None,JKS,TLS,Set(TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA),false,None,None,JKS))
INFO  [dse-app-client-thread-pool-1] 2017-06-12 13:04:16,155  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [dse-app-client-thread-pool-1] 2017-06-12 13:04:16,195  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [dse-app-client-thread-pool-1] 2017-06-12 13:04:16,195  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [dse-app-client-thread-pool-2] 2017-06-12 13:04:16,216  Logging.scala:54 - Connected to Spark cluster with app ID app-20170612130416-0001
INFO  [main] 2017-06-12 13:04:16,223  Logging.scala:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52046.
INFO  [main] 2017-06-12 13:04:16,224  Logging.scala:54 - Server created on 10.150.0.180:52046
INFO  [main] 2017-06-12 13:04:16,229  Logging.scala:54 - external shuffle service port = 7437
INFO  [main] 2017-06-12 13:04:16,230  Logging.scala:54 - Registering BlockManager BlockManagerId(driver, 10.150.0.180, 52046)
INFO  [dispatcher-event-loop-3] 2017-06-12 13:04:16,233  Logging.scala:54 - Registering block manager 10.150.0.180:52046 with 912.3 MB RAM, BlockManagerId(driver, 10.150.0.180, 52046)
INFO  [main] 2017-06-12 13:04:16,237  Logging.scala:54 - Registered BlockManager BlockManagerId(driver, 10.150.0.180, 52046)
INFO  [main] 2017-06-12 13:04:16,411  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@e11ecfa{/metrics/json,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,424  Logging.scala:54 - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
WARN  [main] 2017-06-12 13:04:16,427  Logging.scala:66 - Use an existing SparkContext, some configuration may not take effect.
INFO  [main] 2017-06-12 13:04:16,443  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@4f327096{/SQL,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,444  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@78a515e4{/SQL/json,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,445  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@8cc8cdb{/SQL/execution,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,446  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@44fff386{/SQL/execution/json,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,448  ContextHandler.java:744 - Started o.s.j.s.ServletContextHandler@161aa04a{/static/sql,null,AVAILABLE}
INFO  [main] 2017-06-12 13:04:16,483  Logging.scala:54 - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('dsefs:///user/spark/warehouse').
INFO  [main] 2017-06-12 13:04:16,486  Logging.scala:54 - Warehouse path is 'dsefs:/user/spark/warehouse'.
INFO  [main] 2017-06-12 13:04:17,603  Logging.scala:54 - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
INFO  [main] 2017-06-12 13:04:18,564  HiveMetaStore.java:589 - 0: Opening raw store with implemenation class:com.datastax.bdp.hadoop.hive.metastore.CassandraHiveMetaStore
INFO  [main] 2017-06-12 13:04:18,679  GuavaCompatibility.java:126 - Detected Guava < 19 in the classpath, using legacy compatibility layer
INFO  [main] 2017-06-12 13:04:18,732  DseConnectionUtil.java:175 - Using no credentials
INFO  [main] 2017-06-12 13:04:18,742  Native.java:106 - Could not load JNR C Library, native system calls through this library will not be available (set this logger level to DEBUG to see the full stack trace).
INFO  [main] 2017-06-12 13:04:18,743  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [dispatcher-event-loop-3] 2017-06-12 13:04:18,774  Logging.scala:54 - Registered executor NettyRpcEndpointRef(null) (10.150.0.180:52048) with ID 0
INFO  [dispatcher-event-loop-0] 2017-06-12 13:04:18,837  Logging.scala:54 - Registering block manager 127.0.0.1:52049 with 366.3 MB RAM, BlockManagerId(0, 127.0.0.1, 52049)
WARN  [main] 2017-06-12 13:04:18,962  NettyUtil.java:64 - Found Netty's native epoll transport, but not running on linux-based operating system. Using NIO instead.
INFO  [main] 2017-06-12 13:04:19,464  DCAwareRoundRobinPolicy.java:86 - Using data-center name 'Analytics' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)
INFO  [main] 2017-06-12 13:04:19,466  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 13:04:19,516  HiveMetaStore.java:663 - Added admin role in metastore
INFO  [main] 2017-06-12 13:04:19,517  HiveMetaStore.java:672 - Added public role in metastore
INFO  [main] 2017-06-12 13:04:19,527  HiveMetaStore.java:712 - No user is added in admin role, since config is empty
INFO  [main] 2017-06-12 13:04:19,613  HiveMetaStore.java:746 - 0: get_all_databases
INFO  [main] 2017-06-12 13:04:19,615  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=get_all_databases	
INFO  [main] 2017-06-12 13:04:19,617  SchemaManagerService.java:219 - Updating Cassandra Keyspace to Metastore Database Mapping
INFO  [main] 2017-06-12 13:04:19,617  SchemaManagerService.java:89 - Refresh cluster meta data
INFO  [main] 2017-06-12 13:04:19,618  DseConnectionUtil.java:175 - Using no credentials
INFO  [main] 2017-06-12 13:04:19,618  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 13:04:19,673  DCAwareRoundRobinPolicy.java:86 - Using data-center name 'Analytics' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)
INFO  [main] 2017-06-12 13:04:19,674  Cluster.java:1559 - New Cassandra host /127.0.0.1:9042 added
INFO  [main] 2017-06-12 13:04:19,696  HiveMetaStore.java:746 - 0: get_functions: db=default pat=*
INFO  [main] 2017-06-12 13:04:19,697  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
INFO  [main] 2017-06-12 13:04:19,697  CassandraHiveMetaStore.java:1515 - in getFunctions with dbName: default and functionNamePattern: *
INFO  [main] 2017-06-12 13:04:19,700  HiveMetaStore.java:746 - 0: get_functions: db=dsefs pat=*
INFO  [main] 2017-06-12 13:04:19,701  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=get_functions: db=dsefs pat=*	
INFO  [main] 2017-06-12 13:04:19,701  CassandraHiveMetaStore.java:1515 - in getFunctions with dbName: dsefs and functionNamePattern: *
INFO  [main] 2017-06-12 13:04:19,704  HiveMetaStore.java:746 - 0: get_functions: db=ks pat=*
INFO  [main] 2017-06-12 13:04:19,706  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=get_functions: db=ks pat=*	
INFO  [main] 2017-06-12 13:04:19,706  CassandraHiveMetaStore.java:1515 - in getFunctions with dbName: ks and functionNamePattern: *
INFO  [main] 2017-06-12 13:04:19,852  DseRestClientAuthProviderBuilderFactory.scala:35 - Using no authentication for DSEFS
INFO  [main] 2017-06-12 13:04:20,110  SessionState.java:641 - Created local directory: /var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/e6001707-252c-4fa3-8794-c649b9852e4c_resources
INFO  [main] 2017-06-12 13:04:20,163  SessionState.java:641 - Created HDFS directory: /tmp/hive/russellspitzer/e6001707-252c-4fa3-8794-c649b9852e4c
INFO  [main] 2017-06-12 13:04:20,193  SessionState.java:641 - Created local directory: /var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/russellspitzer/e6001707-252c-4fa3-8794-c649b9852e4c
INFO  [main] 2017-06-12 13:04:20,245  SessionState.java:641 - Created HDFS directory: /tmp/hive/russellspitzer/e6001707-252c-4fa3-8794-c649b9852e4c/_tmp_space.db
INFO  [main] 2017-06-12 13:04:20,267  Logging.scala:54 - Warehouse location for Hive client (version 1.2.1) is dsefs:///user/spark/warehouse
INFO  [main] 2017-06-12 13:04:20,436  SessionState.java:641 - Created local directory: /var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/b15f59a7-a87d-46ad-89f6-6e1308b5962c_resources
INFO  [main] 2017-06-12 13:04:20,477  SessionState.java:641 - Created HDFS directory: /tmp/hive/russellspitzer/b15f59a7-a87d-46ad-89f6-6e1308b5962c
INFO  [main] 2017-06-12 13:04:20,503  SessionState.java:641 - Created local directory: /var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/russellspitzer/b15f59a7-a87d-46ad-89f6-6e1308b5962c
INFO  [main] 2017-06-12 13:04:20,544  SessionState.java:641 - Created HDFS directory: /tmp/hive/russellspitzer/b15f59a7-a87d-46ad-89f6-6e1308b5962c/_tmp_space.db
INFO  [main] 2017-06-12 13:04:20,562  Logging.scala:54 - Warehouse location for Hive client (version 1.2.1) is dsefs:///user/spark/warehouse
INFO  [main] 2017-06-12 13:04:20,740  HiveMetaStore.java:746 - 0: create_database: Database(name:default, description:default database, locationUri:dsefs://127.0.0.1/user/spark/warehouse, parameters:{})
INFO  [main] 2017-06-12 13:04:20,740  HiveMetaStore.java:371 - ugi=russellspitzer	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:dsefs://127.0.0.1/user/spark/warehouse, parameters:{})	
ERROR [main] 2017-06-12 13:04:20,747  RetryingHMSHandler.java:159 - AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy17.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy18.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:280)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:269)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:308)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:51)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:161)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)
	at org.apache.spark.sql.SparkSession.range(SparkSession.scala:509)
	at org.apache.spark.sql.SparkSession.range(SparkSession.scala:484)
	at com.datastax.spark.example.WriteRead$.delayedEndpoint$com$datastax$spark$example$WriteRead$1(WriteRead.scala:32)
	at com.datastax.spark.example.WriteRead$delayedInit$body.apply(WriteRead.scala:11)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.App$$anonfun$main$1.apply(App.scala:76)
	at scala.App$$anonfun$main$1.apply(App.scala:76)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
	at scala.App$class.main(App.scala:76)
	at com.datastax.spark.example.WriteRead$.main(WriteRead.scala:11)
	at com.datastax.spark.example.WriteRead.main(WriteRead.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at scala.reflect.internal.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:70)
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
	at scala.reflect.internal.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:101)
	at scala.reflect.internal.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:70)
	at scala.reflect.internal.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101)
	at scala.tools.nsc.CommonRunner$class.run(ObjectRunner.scala:22)
	at scala.tools.nsc.JarRunner$.run(MainGenericRunner.scala:13)
	at scala.tools.nsc.CommonRunner$class.runAndCatch(ObjectRunner.scala:29)
	at scala.tools.nsc.JarRunner$.runJar(MainGenericRunner.scala:25)
	at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:69)
	at scala.tools.nsc.MainGenericRunner.run$1(MainGenericRunner.scala:87)
	at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:98)
	at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:103)
	at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala)

INFO  [main] 2017-06-12 13:04:21,439  Logging.scala:54 - Code generated in 42.104228 ms
INFO  [main] 2017-06-12 13:04:21,719  Logging.scala:54 - Starting job: runJob at RDDFunctions.scala:36
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:21,731  Logging.scala:54 - Got job 0 (runJob at RDDFunctions.scala:36) with 2 output partitions
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:21,731  Logging.scala:54 - Final stage: ResultStage 0 (runJob at RDDFunctions.scala:36)
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:21,732  Logging.scala:54 - Parents of final stage: List()
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:21,733  Logging.scala:54 - Missing parents: List()
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:21,739  Logging.scala:54 - Submitting ResultStage 0 (MapPartitionsRDD[5] at rdd at WriteRead.scala:34), which has no missing parents
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:21,819  Logging.scala:54 - Block broadcast_0 stored as values in memory (estimated size 18.0 KB, free 912.3 MB)
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:21,842  Logging.scala:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 8.4 KB, free 912.3 MB)
INFO  [dispatcher-event-loop-1] 2017-06-12 13:04:21,844  Logging.scala:54 - Added broadcast_0_piece0 in memory on 10.150.0.180:52046 (size: 8.4 KB, free: 912.3 MB)
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:21,846  Logging.scala:54 - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:21,850  Logging.scala:54 - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at rdd at WriteRead.scala:34)
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:21,852  Logging.scala:54 - Adding task set 0.0 with 2 tasks
INFO  [dispatcher-event-loop-3] 2017-06-12 13:04:21,877  Logging.scala:54 - Starting task 0.0 in stage 0.0 (TID 0, 127.0.0.1, partition 0, PROCESS_LOCAL, 5353 bytes)
INFO  [dispatcher-event-loop-3] 2017-06-12 13:04:21,880  Logging.scala:54 - Starting task 1.0 in stage 0.0 (TID 1, 127.0.0.1, partition 1, PROCESS_LOCAL, 5353 bytes)
INFO  [dispatcher-event-loop-3] 2017-06-12 13:04:21,886  Logging.scala:54 - Launching task 0 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-3] 2017-06-12 13:04:21,888  Logging.scala:54 - Launching task 1 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:22,154  Logging.scala:54 - Added broadcast_0_piece0 in memory on 127.0.0.1:52049 (size: 8.4 KB, free: 366.3 MB)
INFO  [task-result-getter-1] 2017-06-12 13:04:24,560  Logging.scala:54 - Finished task 1.0 in stage 0.0 (TID 1) in 2680 ms on 127.0.0.1 (1/2)
INFO  [task-result-getter-0] 2017-06-12 13:04:24,560  Logging.scala:54 - Finished task 0.0 in stage 0.0 (TID 0) in 2700 ms on 127.0.0.1 (2/2)
INFO  [task-result-getter-0] 2017-06-12 13:04:24,562  Logging.scala:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:24,566  Logging.scala:54 - ResultStage 0 (runJob at RDDFunctions.scala:36) finished in 2.706 s
INFO  [main] 2017-06-12 13:04:24,572  Logging.scala:54 - Job 0 finished: runJob at RDDFunctions.scala:36, took 2.852118 s
INFO  [main] 2017-06-12 13:04:24,782  Logging.scala:54 - Starting job: collect at WriteRead.scala:47
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:24,782  Logging.scala:54 - Got job 1 (collect at WriteRead.scala:47) with 13 output partitions
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:24,783  Logging.scala:54 - Final stage: ResultStage 1 (collect at WriteRead.scala:47)
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:24,783  Logging.scala:54 - Parents of final stage: List()
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:24,783  Logging.scala:54 - Missing parents: List()
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:24,783  Logging.scala:54 - Submitting ResultStage 1 (CassandraTableScanRDD[6] at RDD at CassandraRDD.scala:19), which has no missing parents
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:24,795  Logging.scala:54 - Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 912.3 MB)
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:24,798  Logging.scala:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.3 MB)
INFO  [dispatcher-event-loop-7] 2017-06-12 13:04:24,798  Logging.scala:54 - Added broadcast_1_piece0 in memory on 10.150.0.180:52046 (size: 3.6 KB, free: 912.3 MB)
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:24,799  Logging.scala:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:24,799  Logging.scala:54 - Submitting 13 missing tasks from ResultStage 1 (CassandraTableScanRDD[6] at RDD at CassandraRDD.scala:19)
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:24,799  Logging.scala:54 - Adding task set 1.0 with 13 tasks
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:24,809  Logging.scala:54 - Starting task 0.0 in stage 1.0 (TID 2, 127.0.0.1, partition 0, NODE_LOCAL, 7021 bytes)
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:24,810  Logging.scala:54 - Starting task 1.0 in stage 1.0 (TID 3, 127.0.0.1, partition 1, NODE_LOCAL, 7021 bytes)
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:24,811  Logging.scala:54 - Starting task 2.0 in stage 1.0 (TID 4, 127.0.0.1, partition 2, NODE_LOCAL, 7021 bytes)
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:24,811  Logging.scala:54 - Starting task 3.0 in stage 1.0 (TID 5, 127.0.0.1, partition 3, NODE_LOCAL, 7021 bytes)
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:24,813  Logging.scala:54 - Starting task 4.0 in stage 1.0 (TID 6, 127.0.0.1, partition 4, NODE_LOCAL, 7021 bytes)
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:24,813  Logging.scala:54 - Starting task 5.0 in stage 1.0 (TID 7, 127.0.0.1, partition 5, NODE_LOCAL, 7021 bytes)
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:24,814  Logging.scala:54 - Launching task 2 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:24,814  Logging.scala:54 - Launching task 3 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:24,814  Logging.scala:54 - Launching task 4 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:24,815  Logging.scala:54 - Launching task 5 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:24,815  Logging.scala:54 - Launching task 6 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:24,816  Logging.scala:54 - Launching task 7 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:24,840  Logging.scala:54 - Added broadcast_1_piece0 in memory on 127.0.0.1:52049 (size: 3.6 KB, free: 366.3 MB)
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:24,924  Logging.scala:54 - Starting task 6.0 in stage 1.0 (TID 8, 127.0.0.1, partition 6, NODE_LOCAL, 7021 bytes)
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:24,925  Logging.scala:54 - Launching task 8 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-2] 2017-06-12 13:04:24,925  Logging.scala:54 - Finished task 0.0 in stage 1.0 (TID 2) in 123 ms on 127.0.0.1 (1/13)
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:24,928  Logging.scala:54 - Starting task 7.0 in stage 1.0 (TID 9, 127.0.0.1, partition 7, NODE_LOCAL, 7021 bytes)
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:24,928  Logging.scala:54 - Launching task 9 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-3] 2017-06-12 13:04:24,928  Logging.scala:54 - Finished task 2.0 in stage 1.0 (TID 4) in 118 ms on 127.0.0.1 (2/13)
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:24,931  Logging.scala:54 - Starting task 8.0 in stage 1.0 (TID 10, 127.0.0.1, partition 8, NODE_LOCAL, 7021 bytes)
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:24,931  Logging.scala:54 - Launching task 10 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-1] 2017-06-12 13:04:24,932  Logging.scala:54 - Finished task 4.0 in stage 1.0 (TID 6) in 119 ms on 127.0.0.1 (3/13)
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:24,935  Logging.scala:54 - Starting task 9.0 in stage 1.0 (TID 11, 127.0.0.1, partition 9, NODE_LOCAL, 7021 bytes)
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:24,935  Logging.scala:54 - Launching task 11 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:24,939  Logging.scala:54 - Starting task 10.0 in stage 1.0 (TID 12, 127.0.0.1, partition 10, NODE_LOCAL, 7021 bytes)
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:24,939  Logging.scala:54 - Launching task 12 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:24,941  Logging.scala:54 - Starting task 11.0 in stage 1.0 (TID 13, 127.0.0.1, partition 11, NODE_LOCAL, 7021 bytes)
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:24,941  Logging.scala:54 - Launching task 13 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-0] 2017-06-12 13:04:24,941  Logging.scala:54 - Finished task 5.0 in stage 1.0 (TID 7) in 128 ms on 127.0.0.1 (4/13)
INFO  [task-result-getter-2] 2017-06-12 13:04:24,942  Logging.scala:54 - Finished task 1.0 in stage 1.0 (TID 3) in 133 ms on 127.0.0.1 (5/13)
INFO  [task-result-getter-3] 2017-06-12 13:04:24,942  Logging.scala:54 - Finished task 3.0 in stage 1.0 (TID 5) in 131 ms on 127.0.0.1 (6/13)
INFO  [dispatcher-event-loop-4] 2017-06-12 13:04:24,952  Logging.scala:54 - Starting task 12.0 in stage 1.0 (TID 14, 127.0.0.1, partition 12, NODE_LOCAL, 7021 bytes)
INFO  [dispatcher-event-loop-4] 2017-06-12 13:04:24,953  Logging.scala:54 - Launching task 14 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-1] 2017-06-12 13:04:24,953  Logging.scala:54 - Finished task 6.0 in stage 1.0 (TID 8) in 30 ms on 127.0.0.1 (7/13)
INFO  [task-result-getter-0] 2017-06-12 13:04:24,955  Logging.scala:54 - Finished task 10.0 in stage 1.0 (TID 12) in 19 ms on 127.0.0.1 (8/13)
INFO  [task-result-getter-2] 2017-06-12 13:04:24,956  Logging.scala:54 - Finished task 11.0 in stage 1.0 (TID 13) in 16 ms on 127.0.0.1 (9/13)
INFO  [task-result-getter-3] 2017-06-12 13:04:24,959  Logging.scala:54 - Finished task 7.0 in stage 1.0 (TID 9) in 33 ms on 127.0.0.1 (10/13)
INFO  [task-result-getter-1] 2017-06-12 13:04:24,963  Logging.scala:54 - Finished task 9.0 in stage 1.0 (TID 11) in 31 ms on 127.0.0.1 (11/13)
INFO  [task-result-getter-0] 2017-06-12 13:04:24,966  Logging.scala:54 - Finished task 12.0 in stage 1.0 (TID 14) in 15 ms on 127.0.0.1 (12/13)
INFO  [task-result-getter-2] 2017-06-12 13:04:24,974  Logging.scala:54 - Finished task 8.0 in stage 1.0 (TID 10) in 46 ms on 127.0.0.1 (13/13)
INFO  [task-result-getter-2] 2017-06-12 13:04:24,975  Logging.scala:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:24,975  Logging.scala:54 - ResultStage 1 (collect at WriteRead.scala:47) finished in 0.173 s
INFO  [main] 2017-06-12 13:04:24,975  Logging.scala:54 - Job 1 finished: collect at WriteRead.scala:47, took 0.193306 s
INFO  [main] 2017-06-12 13:04:24,994  Logging.scala:35 - Input Predicates: []
INFO  [main] 2017-06-12 13:04:24,996  Clock.java:46 - Using java.lang.System clock to generate timestamps.
INFO  [main] 2017-06-12 13:04:25,042  Cluster.java:1559 - New Cassandra host localhost/127.0.0.1:9042 added
INFO  [main] 2017-06-12 13:04:25,043  Logging.scala:35 - Connected to Cassandra cluster: Test Cluster
INFO  [main] 2017-06-12 13:04:25,112  Logging.scala:35 - Input Predicates: []
INFO  [dispatcher-event-loop-2] 2017-06-12 13:04:25,283  Logging.scala:54 - Removed broadcast_0_piece0 on 10.150.0.180:52046 in memory (size: 8.4 KB, free: 912.3 MB)
INFO  [dispatcher-event-loop-5] 2017-06-12 13:04:25,292  Logging.scala:54 - Removed broadcast_0_piece0 on 127.0.0.1:52049 in memory (size: 8.4 KB, free: 366.3 MB)
INFO  [dispatcher-event-loop-3] 2017-06-12 13:04:25,301  Logging.scala:54 - Removed broadcast_1_piece0 on 10.150.0.180:52046 in memory (size: 3.6 KB, free: 912.3 MB)
INFO  [dispatcher-event-loop-4] 2017-06-12 13:04:25,303  Logging.scala:54 - Removed broadcast_1_piece0 on 127.0.0.1:52049 in memory (size: 3.6 KB, free: 366.3 MB)
INFO  [main] 2017-06-12 13:04:25,319  Logging.scala:54 - Code generated in 21.484792 ms
INFO  [main] 2017-06-12 13:04:25,367  Logging.scala:54 - Starting job: collect at WriteRead.scala:51
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:25,368  Logging.scala:54 - Got job 2 (collect at WriteRead.scala:51) with 13 output partitions
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:25,368  Logging.scala:54 - Final stage: ResultStage 2 (collect at WriteRead.scala:51)
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:25,368  Logging.scala:54 - Parents of final stage: List()
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:25,368  Logging.scala:54 - Missing parents: List()
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:25,369  Logging.scala:54 - Submitting ResultStage 2 (MapPartitionsRDD[11] at collect at WriteRead.scala:51), which has no missing parents
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:25,376  Logging.scala:54 - Block broadcast_2 stored as values in memory (estimated size 11.1 KB, free 912.3 MB)
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:25,378  Logging.scala:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.8 KB, free 912.3 MB)
INFO  [dispatcher-event-loop-7] 2017-06-12 13:04:25,379  Logging.scala:54 - Added broadcast_2_piece0 in memory on 10.150.0.180:52046 (size: 5.8 KB, free: 912.3 MB)
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:25,380  Logging.scala:54 - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:25,380  Logging.scala:54 - Submitting 13 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at collect at WriteRead.scala:51)
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:25,380  Logging.scala:54 - Adding task set 2.0 with 13 tasks
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:25,383  Logging.scala:54 - Starting task 0.0 in stage 2.0 (TID 15, 127.0.0.1, partition 0, NODE_LOCAL, 7050 bytes)
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:25,385  Logging.scala:54 - Starting task 1.0 in stage 2.0 (TID 16, 127.0.0.1, partition 1, NODE_LOCAL, 7050 bytes)
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:25,386  Logging.scala:54 - Starting task 2.0 in stage 2.0 (TID 17, 127.0.0.1, partition 2, NODE_LOCAL, 7050 bytes)
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:25,388  Logging.scala:54 - Starting task 3.0 in stage 2.0 (TID 18, 127.0.0.1, partition 3, NODE_LOCAL, 7050 bytes)
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:25,390  Logging.scala:54 - Starting task 4.0 in stage 2.0 (TID 19, 127.0.0.1, partition 4, NODE_LOCAL, 7050 bytes)
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:25,391  Logging.scala:54 - Starting task 5.0 in stage 2.0 (TID 20, 127.0.0.1, partition 5, NODE_LOCAL, 7050 bytes)
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:25,392  Logging.scala:54 - Launching task 15 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:25,392  Logging.scala:54 - Launching task 16 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:25,392  Logging.scala:54 - Launching task 17 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:25,393  Logging.scala:54 - Launching task 18 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:25,393  Logging.scala:54 - Launching task 19 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:25,394  Logging.scala:54 - Launching task 20 on executor id: 0 hostname: 127.0.0.1.
INFO  [dispatcher-event-loop-6] 2017-06-12 13:04:25,407  Logging.scala:54 - Added broadcast_2_piece0 in memory on 127.0.0.1:52049 (size: 5.8 KB, free: 366.3 MB)
INFO  [dispatcher-event-loop-2] 2017-06-12 13:04:25,475  Logging.scala:54 - Starting task 6.0 in stage 2.0 (TID 21, 127.0.0.1, partition 6, NODE_LOCAL, 7050 bytes)
INFO  [dispatcher-event-loop-2] 2017-06-12 13:04:25,475  Logging.scala:54 - Launching task 21 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-3] 2017-06-12 13:04:25,475  Logging.scala:54 - Finished task 4.0 in stage 2.0 (TID 19) in 87 ms on 127.0.0.1 (1/13)
INFO  [dispatcher-event-loop-2] 2017-06-12 13:04:25,476  Logging.scala:54 - Starting task 7.0 in stage 2.0 (TID 22, 127.0.0.1, partition 7, NODE_LOCAL, 7050 bytes)
INFO  [dispatcher-event-loop-2] 2017-06-12 13:04:25,477  Logging.scala:54 - Launching task 22 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-1] 2017-06-12 13:04:25,477  Logging.scala:54 - Finished task 2.0 in stage 2.0 (TID 17) in 92 ms on 127.0.0.1 (2/13)
INFO  [dispatcher-event-loop-0] 2017-06-12 13:04:25,479  Logging.scala:54 - Starting task 8.0 in stage 2.0 (TID 23, 127.0.0.1, partition 8, NODE_LOCAL, 7050 bytes)
INFO  [dispatcher-event-loop-0] 2017-06-12 13:04:25,480  Logging.scala:54 - Launching task 23 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-0] 2017-06-12 13:04:25,480  Logging.scala:54 - Finished task 0.0 in stage 2.0 (TID 15) in 99 ms on 127.0.0.1 (3/13)
INFO  [dispatcher-event-loop-7] 2017-06-12 13:04:25,484  Logging.scala:54 - Starting task 9.0 in stage 2.0 (TID 24, 127.0.0.1, partition 9, NODE_LOCAL, 7050 bytes)
INFO  [dispatcher-event-loop-7] 2017-06-12 13:04:25,484  Logging.scala:54 - Launching task 24 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-2] 2017-06-12 13:04:25,485  Logging.scala:54 - Finished task 5.0 in stage 2.0 (TID 20) in 95 ms on 127.0.0.1 (4/13)
INFO  [dispatcher-event-loop-7] 2017-06-12 13:04:25,486  Logging.scala:54 - Starting task 10.0 in stage 2.0 (TID 25, 127.0.0.1, partition 10, NODE_LOCAL, 7050 bytes)
INFO  [dispatcher-event-loop-7] 2017-06-12 13:04:25,486  Logging.scala:54 - Launching task 25 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-3] 2017-06-12 13:04:25,486  Logging.scala:54 - Finished task 3.0 in stage 2.0 (TID 18) in 100 ms on 127.0.0.1 (5/13)
INFO  [dispatcher-event-loop-7] 2017-06-12 13:04:25,487  Logging.scala:54 - Starting task 11.0 in stage 2.0 (TID 26, 127.0.0.1, partition 11, NODE_LOCAL, 7050 bytes)
INFO  [dispatcher-event-loop-7] 2017-06-12 13:04:25,488  Logging.scala:54 - Launching task 26 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-1] 2017-06-12 13:04:25,488  Logging.scala:54 - Finished task 1.0 in stage 2.0 (TID 16) in 104 ms on 127.0.0.1 (6/13)
INFO  [dispatcher-event-loop-3] 2017-06-12 13:04:25,493  Logging.scala:54 - Starting task 12.0 in stage 2.0 (TID 27, 127.0.0.1, partition 12, NODE_LOCAL, 7050 bytes)
INFO  [dispatcher-event-loop-3] 2017-06-12 13:04:25,493  Logging.scala:54 - Launching task 27 on executor id: 0 hostname: 127.0.0.1.
INFO  [task-result-getter-0] 2017-06-12 13:04:25,493  Logging.scala:54 - Finished task 7.0 in stage 2.0 (TID 22) in 17 ms on 127.0.0.1 (7/13)
INFO  [task-result-getter-2] 2017-06-12 13:04:25,494  Logging.scala:54 - Finished task 8.0 in stage 2.0 (TID 23) in 16 ms on 127.0.0.1 (8/13)
INFO  [task-result-getter-3] 2017-06-12 13:04:25,501  Logging.scala:54 - Finished task 6.0 in stage 2.0 (TID 21) in 28 ms on 127.0.0.1 (9/13)
INFO  [task-result-getter-1] 2017-06-12 13:04:25,502  Logging.scala:54 - Finished task 11.0 in stage 2.0 (TID 26) in 14 ms on 127.0.0.1 (10/13)
INFO  [task-result-getter-0] 2017-06-12 13:04:25,502  Logging.scala:54 - Finished task 10.0 in stage 2.0 (TID 25) in 17 ms on 127.0.0.1 (11/13)
INFO  [task-result-getter-2] 2017-06-12 13:04:25,503  Logging.scala:54 - Finished task 9.0 in stage 2.0 (TID 24) in 20 ms on 127.0.0.1 (12/13)
INFO  [task-result-getter-3] 2017-06-12 13:04:25,505  Logging.scala:54 - Finished task 12.0 in stage 2.0 (TID 27) in 13 ms on 127.0.0.1 (13/13)
INFO  [task-result-getter-3] 2017-06-12 13:04:25,506  Logging.scala:54 - Removed TaskSet 2.0, whose tasks have all completed, from pool 
INFO  [dag-scheduler-event-loop] 2017-06-12 13:04:25,506  Logging.scala:54 - ResultStage 2 (collect at WriteRead.scala:51) finished in 0.124 s
INFO  [main] 2017-06-12 13:04:25,506  Logging.scala:54 - Job 2 finished: collect at WriteRead.scala:51, took 0.138924 s
INFO  [main] 2017-06-12 13:04:25,538  Logging.scala:54 - Code generated in 13.511444 ms
INFO  [main] 2017-06-12 13:04:25,561  AbstractConnector.java:306 - Stopped ServerConnector@4e1a46fb{HTTP/1.1}{0.0.0.0:4040}
INFO  [main] 2017-06-12 13:04:25,564  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5a6d30e2{/stages/stage/kill,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,564  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@151bf776{/api,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,565  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@1cd3b138{/,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,565  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4faa298{/static,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,565  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@121bb45b{/executors/threadDump/json,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,566  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@15639440{/executors/threadDump,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,566  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@32e54a9d{/executors/json,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,566  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@712cfb63{/executors,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,567  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@7100dea{/environment/json,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,567  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@726aa968{/environment,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,567  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4d21c56e{/storage/rdd/json,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,568  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@36681447{/storage/rdd,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,568  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5624657a{/storage/json,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,568  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@77b919a3{/storage,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,569  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@28da7d11{/stages/pool/json,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,569  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4d95a72e{/stages/pool,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,569  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5fcfde70{/stages/stage/json,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,569  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@44536de4{/stages/stage,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,570  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@4b65d9f4{/stages/json,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,570  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@54d8c20d{/stages,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,570  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@2e66bc32{/jobs/job/json,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,571  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@799ed4e8{/jobs/job,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,571  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@5af8bb51{/jobs/json,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,571  ContextHandler.java:865 - Stopped o.s.j.s.ServletContextHandler@c3edf4c{/jobs,null,UNAVAILABLE}
INFO  [main] 2017-06-12 13:04:25,573  Logging.scala:54 - Stopped Spark web UI at http://10.150.0.180:4040
INFO  [main] 2017-06-12 13:04:25,587  Logging.scala:54 - Shutting down all executors
INFO  [dispatcher-event-loop-4] 2017-06-12 13:04:25,588  Logging.scala:54 - Asking each executor to shut down
INFO  [pool-1-thread-1] 2017-06-12 13:04:28,713  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [dispatcher-event-loop-0] 2017-06-12 13:04:30,677  Logging.scala:54 - Shutting down the thread pool
INFO  [dispatcher-event-loop-0] 2017-06-12 13:04:30,678  Logging.scala:54 - Evicting connector cache
INFO  [dispatcher-event-loop-1] 2017-06-12 13:04:30,684  Logging.scala:54 - MapOutputTrackerMasterEndpoint stopped!
INFO  [main] 2017-06-12 13:04:30,694  Logging.scala:54 - MemoryStore cleared
INFO  [main] 2017-06-12 13:04:30,695  Logging.scala:54 - BlockManager stopped
INFO  [main] 2017-06-12 13:04:30,697  Logging.scala:54 - BlockManagerMaster stopped
INFO  [dispatcher-event-loop-2] 2017-06-12 13:04:30,700  Logging.scala:54 - OutputCommitCoordinator stopped!
INFO  [pool-1-thread-1] 2017-06-12 13:04:32,097  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [dispatcher-event-loop-0] 2017-06-12 13:04:32,900  Logging.scala:35 - Disconnected from Cassandra cluster: Test Cluster
INFO  [dispatcher-event-loop-0] 2017-06-12 13:04:32,900  Logging.scala:54 - onStop complete
INFO  [main] 2017-06-12 13:04:32,902  Logging.scala:54 - Successfully stopped SparkContext
INFO  [Thread-2] 2017-06-12 13:04:32,905  Logging.scala:54 - Shutdown hook called
INFO  [Serial shutdown hooks thread] 2017-06-12 13:04:32,905  Logging.scala:35 - Successfully executed shutdown hook: Clearing session cache for C* connector
INFO  [Thread-2] 2017-06-12 13:04:32,906  Logging.scala:54 - Deleting directory /private/var/folders/6q/7bdfdh4j0ds1lq41gp5_3s800000gn/T/spark-d2c0ffec-074c-417e-9fb3-9e0205a9c7b9
